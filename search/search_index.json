{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Transformer-Introduction HuggingFace ecosystem Transformers: https://github.com/huggingface/transformers Datasets: https://github.com/huggingface/datasets Tokenizers: https://github.com/huggingface/tokenizers Accelerate: https://github.com/huggingface/accelerate HuggingFace Hub: https://huggingface.co/models Common NLP tasks Classifying whole sentences: sentiment analysis, spam classification, verify sentence grammer, logical relation between sentences Classifying each word in a sentence: NER, pos tag Generating text content: Text generation, fill in the blanks with masked words Extracting an answer from a text: QnA Generating a new sentence from an input text: NMT, text summarization","title":"Introduction"},{"location":"#transformer-introduction","text":"HuggingFace ecosystem Transformers: https://github.com/huggingface/transformers Datasets: https://github.com/huggingface/datasets Tokenizers: https://github.com/huggingface/tokenizers Accelerate: https://github.com/huggingface/accelerate HuggingFace Hub: https://huggingface.co/models Common NLP tasks Classifying whole sentences: sentiment analysis, spam classification, verify sentence grammer, logical relation between sentences Classifying each word in a sentence: NER, pos tag Generating text content: Text generation, fill in the blanks with masked words Extracting an answer from a text: QnA Generating a new sentence from an input text: NMT, text summarization","title":"Transformer-Introduction"},{"location":"1tranformer-pipeline/","text":"Pipeline Function The pipeline function returns an end-to-end object that performs an NLP task on one or several texts. It includes steps: Pre-Processing Model Post-Processing from transformers import pipeline classifier = pipeline ( \"sentiment-analysis\" ) classifier ( \"I've been waiting for a HuggingFace course my whole life.\" ) By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again. Available pipelines are: feature-extraction (get the vector representation of a text) fill-mask: to fill in the blanks ner (named entity recognition): identifies persons, locations, or organizations question-answering: answers questions using information from a given context sentiment-analysis summarization: reducing a text into a shorter text while keeping almost of the important aspects referenced in the text text-generation translation zero-shot-classification: classify on the basis of listed classes/labels. Transformer Models: June 2018: GPT October 2018: BERT February 2019: GPT-2 October 2019: DistilBERT , a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT\u2019s performance. October 2019: BART and T5 : Both Encoder and Decoder May 2020: GPT-3 : an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning) Transformer Model Categories: GPT-like (also called auto-regressive Transformer models) BERT-like (also called auto-encoding Transformer models) BART/T5-like (also called sequence-to-sequence Transformer models) Transformers are: Language models: trained on large amounts of raw text in a self-supervised fashion Self-supervised learner: here objective is automatically computed from the inputs of the model pretrained model goes through transfer learning, ie, fine-tuned in supervised way using human-annotated labels on given task. Casual Language Modeling: output depends on the past and present inputs, but not the future ones Masked language modeling: predicts a masked word in the sentence Transfer Learning The act of initializing a model with another model's weights. Training from scratch requires more data and more compute to achieve comparable results. In NLP, predicting the next word is a common pretraining objective.(GPT) Another common pretraining objective in text is to gues the value of randomly masked words.(BERT) Usually, Transfer Learning is applied by dropping the head of the pretrained model while keeping its body. The pretrained model helps by transferring its knowledge but it also transfers the bias it may contain. OpenAI studied the bias predictions of its GPT-3 model. pre-training fine-tuning training from scratch transfer learning Architectures vs. checkpoints Architecture: This is the skeleton of the model \u2014 the definition of each layer and each operation that happens within the model. Checkpoints: These are the weights that will be loaded in a given architecture. Model: This is an umbrella term that isn\u2019t as precise as \u201carchitecture\u201d or \u201ccheckpoint\u201d: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity. Bert Family: ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa Decoder family: CTRL, GPT, GPT-2, Transformer XL, GPT Neo Encoder-decoder(sequence-to-sequence) models: BART, mBART, Marian, T5, Pegasus, ProphetNet, M2M100","title":"Pipeline"},{"location":"1tranformer-pipeline/#bert-family","text":"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa","title":"Bert Family:"},{"location":"1tranformer-pipeline/#decoder-family","text":"CTRL, GPT, GPT-2, Transformer XL, GPT Neo","title":"Decoder family:"},{"location":"1tranformer-pipeline/#encoder-decodersequence-to-sequence-models","text":"BART, mBART, Marian, T5, Pegasus, ProphetNet, M2M100","title":"Encoder-decoder(sequence-to-sequence) models:"},{"location":"2UsingTransformerIntro/","text":"Introduction In this section we will: Learn the basic building blocks of a Transformer model. Learn what makes up a tokenization pipeline. See how to use a Transformer model in practice. Learn how to leverage a tokenizer to convert text to tensors that are understandable by the model. Set up a tokenizer and a model together to get from text to predictions. Learn the limitations of input IDs, and learned about attention masks. Play around with versatile and configurable tokenizer methods.","title":"Introduction"},{"location":"2UsingTransformerIntro/#introduction","text":"In this section we will: Learn the basic building blocks of a Transformer model. Learn what makes up a tokenization pipeline. See how to use a Transformer model in practice. Learn how to leverage a tokenizer to convert text to tensors that are understandable by the model. Set up a tokenizer and a model together to get from text to predictions. Learn the limitations of input IDs, and learned about attention masks. Play around with versatile and configurable tokenizer methods.","title":"Introduction"},{"location":"2_1BehindPipeline/","text":"Behind the pipeline Preprocessing with a tokenizer -> All preprocessing needs to be done in exactly the same way as when the model was pretrained. -> To do this, we use the AutoTokenizer class and its from_pretrained() method. -> Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it. from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) -> Next step is to convert the list of input IDs to tensors. -> To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument: raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this so much!\" , ] inputs = tokenizer ( raw_inputs , padding = True , truncation = True , return_tensors = \"pt\" ) #return_tensors: To specify the type of tensors we want to get back # truncation=True, Any sentence longer than the maximum the model can handle is truncated print ( inputs ) -> The output itself is a dictionary containing two keys, input_ids and attention_mask. -> input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. -> attention mask indicates where padding has been applied, so the model does not pay attention to it. Going through the model -> Pretrained model can be downloaded similarly as tokenizer using AutoModel class. -> AutoModel class loads a model without its pretraining head. from transformers import AutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModel . from_pretrained ( checkpoint ) # downloads configuration of the model as well as pre-trained weights, only initantiates the body of the model # downloaded same checkpoint used in pipeline(cached already) and instantiated a model with it -> This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call hidden states, also known as features. For each model input, we\u2019ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model. -> While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the head. Different NLP tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it. A high-dimensional vector? The vector output by the Transformer module is usually large. It generally has three dimensions: Batch size: The number of sequences processed at a time (2 in our example). Sequence length: The length of the numerical representation of the sequence (16 in our example). Hidden size: The vector dimension of each model input. It is said to be \u201chigh dimensional\u201d because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more). outputs = model ( ** inputs ) print ( outputs . last_hidden_state . shape ) # torch.Size([2, 16, 768]) Note that the outputs of Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]). Model heads: Making sense out of numbers The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. The output of the Transformer model is sent directly to the model head to be processed. There are many different architectures available in Transformers, with each one designed around tackling a specific task: *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification, etc For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the AutoModel class, but AutoModelForSequenceClassification: from transformers import AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** inputs ) print ( outputs . logits . shape ) # torch.Size([2, 2]) - Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): - Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2. Postprocessing the output print ( outputs . logits ) logits are raw, unnormalized scores outputted by the last layer of the model. - To be converted to probabilities, they need to go through a SoftMax layer (all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy) import torch predictions = torch . nn . functional . softmax ( outputs . logits , dim =- 1 ) print ( predictions ) It returns probability scores. To get the labels corresponding to each position, we can inspect the id2label attribute of the model config. model . config . id2label # {0: 'NEGATIVE', 1: 'POSITIVE'}","title":"Behind The Pipeline"},{"location":"2_1BehindPipeline/#behind-the-pipeline","text":"","title":"Behind the pipeline"},{"location":"2_1BehindPipeline/#preprocessing-with-a-tokenizer","text":"-> All preprocessing needs to be done in exactly the same way as when the model was pretrained. -> To do this, we use the AutoTokenizer class and its from_pretrained() method. -> Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it. from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) -> Next step is to convert the list of input IDs to tensors. -> To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument: raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this so much!\" , ] inputs = tokenizer ( raw_inputs , padding = True , truncation = True , return_tensors = \"pt\" ) #return_tensors: To specify the type of tensors we want to get back # truncation=True, Any sentence longer than the maximum the model can handle is truncated print ( inputs ) -> The output itself is a dictionary containing two keys, input_ids and attention_mask. -> input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. -> attention mask indicates where padding has been applied, so the model does not pay attention to it.","title":"Preprocessing with a tokenizer"},{"location":"2_1BehindPipeline/#going-through-the-model","text":"-> Pretrained model can be downloaded similarly as tokenizer using AutoModel class. -> AutoModel class loads a model without its pretraining head. from transformers import AutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModel . from_pretrained ( checkpoint ) # downloads configuration of the model as well as pre-trained weights, only initantiates the body of the model # downloaded same checkpoint used in pipeline(cached already) and instantiated a model with it -> This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call hidden states, also known as features. For each model input, we\u2019ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model. -> While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the head. Different NLP tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.","title":"Going through the model"},{"location":"2_1BehindPipeline/#a-high-dimensional-vector","text":"The vector output by the Transformer module is usually large. It generally has three dimensions: Batch size: The number of sequences processed at a time (2 in our example). Sequence length: The length of the numerical representation of the sequence (16 in our example). Hidden size: The vector dimension of each model input. It is said to be \u201chigh dimensional\u201d because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more). outputs = model ( ** inputs ) print ( outputs . last_hidden_state . shape ) # torch.Size([2, 16, 768]) Note that the outputs of Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).","title":"A high-dimensional vector?"},{"location":"2_1BehindPipeline/#model-heads-making-sense-out-of-numbers","text":"The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. The output of the Transformer model is sent directly to the model head to be processed. There are many different architectures available in Transformers, with each one designed around tackling a specific task: *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification, etc For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the AutoModel class, but AutoModelForSequenceClassification: from transformers import AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** inputs ) print ( outputs . logits . shape ) # torch.Size([2, 2]) - Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): - Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.","title":"Model heads: Making sense out of numbers"},{"location":"2_1BehindPipeline/#postprocessing-the-output","text":"print ( outputs . logits ) logits are raw, unnormalized scores outputted by the last layer of the model. - To be converted to probabilities, they need to go through a SoftMax layer (all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy) import torch predictions = torch . nn . functional . softmax ( outputs . logits , dim =- 1 ) print ( predictions ) It returns probability scores. To get the labels corresponding to each position, we can inspect the id2label attribute of the model config. model . config . id2label # {0: 'NEGATIVE', 1: 'POSITIVE'}","title":"Postprocessing the output"},{"location":"2_2Models/","text":"Instantiate a Transformers model AutoModel API allows you to instantiate a pretrained model from any checkpoint. from transformers import AutoModel bert_model = AutoModel . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_model )) gpt_model = AutoModel . from_pretrained ( \"gpt2\" ) print ( type ( gpt_model )) bart_model = AutoModel . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_model )) It will pick the right model class from the library to instantiate the proper architecture and loads the weights of the pre-trained model inside. Checkpoint or local folder Behind the AutoModel.from_pretrained() method: config.json file: attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what Transformers version you were using when you last saved the checkpoint. pytorch_model.bin file: known as the state dictionary; it contains all your model\u2019s weights configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters. The weights can be downloaded and cached (so future calls to the from_pretrained() method won\u2019t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable. To instantiate a pre-trained model, the AutoConfig API will first check in config file to look at the config class that should be used. The config class depends on the type of model(bert, gpt-2, etc). Once it attaches a proper config class, it can instantiate that configuration which is a blueprint to know how to create a model. It uses this configuration class to find the proper model which is combined to the logit configuration to load the model. this model is not yet trained model as it just being initialized with the random weights. The last step is to loads weight from the model file inside this model(above loaded model). The AutoConfig API allows you to instantiate the configuration of a pretrained model from any checkpoint: from transformers import AutoConfig bert_config = AutoConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) gpt_config = AutoConfig . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) bart_config = AutoConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) But you can also use the specific class if you know it: from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) from transformers import GPT2Config gpt_config = GPT2Config . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) from transformers import BartConfig bart_config = BartConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) The configuration(bert_config, gpt_config, bart_config) contains all the information needed to load the model/create the model architecture. from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( bert_config ) Then you can instantiate a given model with random weights from this config. ie, once we have the configuration we can create a model which has same architecture as the checkpoint which is from it was initialized. We can train it from scratch. We can also change any part of its configurations using keyword arguments # Same architecture as bert-base-cased from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = BertModel ( bert_config ) # Using only 10 layers instead of 12 from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" , num_hidden_layers = 10 ) bert_model = BertModel ( bert_config ) Saving a model To save a model, we just have to use the the save_pretrained method. from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = BertModel ( bert_config ) # Training code bert_model . save_pretrained ( \"my-bert-model\" ) Reloading a saved model from transformers import BertModel bert_model = BertModel . from_pretrained ( \"my-bert-model\" )","title":"Models"},{"location":"2_2Models/#instantiate-a-transformers-model","text":"AutoModel API allows you to instantiate a pretrained model from any checkpoint. from transformers import AutoModel bert_model = AutoModel . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_model )) gpt_model = AutoModel . from_pretrained ( \"gpt2\" ) print ( type ( gpt_model )) bart_model = AutoModel . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_model )) It will pick the right model class from the library to instantiate the proper architecture and loads the weights of the pre-trained model inside.","title":"Instantiate a Transformers model"},{"location":"2_2Models/#checkpoint-or-local-folder","text":"Behind the AutoModel.from_pretrained() method: config.json file: attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what Transformers version you were using when you last saved the checkpoint. pytorch_model.bin file: known as the state dictionary; it contains all your model\u2019s weights configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters. The weights can be downloaded and cached (so future calls to the from_pretrained() method won\u2019t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable. To instantiate a pre-trained model, the AutoConfig API will first check in config file to look at the config class that should be used. The config class depends on the type of model(bert, gpt-2, etc). Once it attaches a proper config class, it can instantiate that configuration which is a blueprint to know how to create a model. It uses this configuration class to find the proper model which is combined to the logit configuration to load the model. this model is not yet trained model as it just being initialized with the random weights. The last step is to loads weight from the model file inside this model(above loaded model). The AutoConfig API allows you to instantiate the configuration of a pretrained model from any checkpoint: from transformers import AutoConfig bert_config = AutoConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) gpt_config = AutoConfig . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) bart_config = AutoConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) But you can also use the specific class if you know it: from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) from transformers import GPT2Config gpt_config = GPT2Config . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) from transformers import BartConfig bart_config = BartConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) The configuration(bert_config, gpt_config, bart_config) contains all the information needed to load the model/create the model architecture. from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( bert_config ) Then you can instantiate a given model with random weights from this config. ie, once we have the configuration we can create a model which has same architecture as the checkpoint which is from it was initialized. We can train it from scratch. We can also change any part of its configurations using keyword arguments # Same architecture as bert-base-cased from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = BertModel ( bert_config ) # Using only 10 layers instead of 12 from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" , num_hidden_layers = 10 ) bert_model = BertModel ( bert_config )","title":"Checkpoint or local folder"},{"location":"2_2Models/#saving-a-model","text":"To save a model, we just have to use the the save_pretrained method. from transformers import BertConfig , BertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = BertModel ( bert_config ) # Training code bert_model . save_pretrained ( \"my-bert-model\" )","title":"Saving a model"},{"location":"2_2Models/#reloading-a-saved-model","text":"from transformers import BertModel bert_model = BertModel . from_pretrained ( \"my-bert-model\" )","title":"Reloading a saved model"},{"location":"2_3Tokenizers/","text":"Tokenizers: In NLP, most of the data we handle is raw text. Tokenizers is used to transform raw text to numbers. Tokenizer's objective is to find a meaningful representation. 3 distict tokenizations: Word-based Character-based Subword-based Word-based The text is split on spaces Other rules, such as punctuation, may be added In this algorithm, each word has a specific ID/number attributed to it Limits: very similar words have entirely different meanings. eg: dog vs dogs the vocabulary can end up very large due to lot of different words large vocabularies result in heavy models loss of meaning across very similar words large quantity of out-of-vocabulary tokens We can limit the amount of words we add to the vocabulary by ignoring certain words which are not necessary Out of vocabulary words result in a loss of information, since model will have the exact same representation for all words that it doesn't know, [UNK], unknown. tokenized_text = \"Jim Henson was a puppeteer\" . split () print ( tokenized_text ) # ['Jim', 'Henson', 'was', 'a', 'puppeteer'] Character-based Splitting a raw text into characters Character-based vocabularies are slimmer There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. Limits: intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own in compared to any word Their sequences are translated into very large amounts of tokens to be processed by the model and this can have an impact on the size of the contants the model will carry around and it will reduce the size of the text we can use as input for a model which is often limited very long sequences less meaningful individual tokens Subword tokenization Splitting a raw text into subwords Finding a middle ground between word and character-based algorithms Subword-based tokenization lies between character and worf-based algorithm Frequently used words should not be split into smaller subwords Rare words should be decomposed into meaningful subwords Subwords help identify similar syntactic or semantic situations in text Subword tokenization algorithms can identify start of word tokens and which tokens complete start of words Most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. WordPiece: BERT, DistilBERT Unigram: XLNet, ALBERT Byte-Pair Encoding: GPT-2, RoBERTa These approaches help in reducing the vocabulary sizes by sharing information across different words having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words by recognizing similar tokens making them up. Tokenizer Pipeline A tokenizer takes texts as inputs and outputs numbers the associated model can make sense of from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Tokenization pipeline: from input text to a list of numbers: Raw text: Let's try to tokenize! Tokens: [let, ', s, try, to, token, ##ize, !] Special tokens: [[CLS],let,',s, try, to, token, ##ize,!,[SEP]] Input IDs: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] The first step of the pipeline is to split the text into tokens. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[let, ', s, try, to, token, ##ize, !] Albert tokenize will add a long underscore in front of all the tokens that added space before them which is a convention shared by all sentence-based tokenizers from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"albert-base-v1\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[_let, ', s, _try, _to, _to, ken, ize, !] The second step of the tokenization pipeline is to map those tokens to their respective input ids as defined by the vocabulary of the tokenizer. This is why we need to download the file when we instantiate a tokenizer with from_pretrained() method. We have to make sure we use the same mapping as when the model was pretrained. To do this, we use convert_tokens_to_ids() method: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( input_ids ) #[2292, 1005, 1055, 3046, 2000, 19204, 4697, 999] Lastly, the tokenizer adds special tokens the model expects by prepare_for_model() method: final_inputs = tokenizer . prepare_for_model ( input_ids ) print ( final_inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Decoding The decode method allows us to check how the final output of the tokenizer translates back into text. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"[CLS] let's try to tokenize! [SEP]\" Roberta tokenizers uses special tokens as : & from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"roberta\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"<s> let's try to tokenize! </s>\" Hence, a tokenizer takes texts as inputs and outputs numbers the associated model can make sense of. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) # {'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}","title":"Tokenizers"},{"location":"2_3Tokenizers/#tokenizers","text":"In NLP, most of the data we handle is raw text. Tokenizers is used to transform raw text to numbers. Tokenizer's objective is to find a meaningful representation. 3 distict tokenizations: Word-based Character-based Subword-based","title":"Tokenizers:"},{"location":"2_3Tokenizers/#word-based","text":"The text is split on spaces Other rules, such as punctuation, may be added In this algorithm, each word has a specific ID/number attributed to it Limits: very similar words have entirely different meanings. eg: dog vs dogs the vocabulary can end up very large due to lot of different words large vocabularies result in heavy models loss of meaning across very similar words large quantity of out-of-vocabulary tokens We can limit the amount of words we add to the vocabulary by ignoring certain words which are not necessary Out of vocabulary words result in a loss of information, since model will have the exact same representation for all words that it doesn't know, [UNK], unknown. tokenized_text = \"Jim Henson was a puppeteer\" . split () print ( tokenized_text ) # ['Jim', 'Henson', 'was', 'a', 'puppeteer']","title":"Word-based"},{"location":"2_3Tokenizers/#character-based","text":"Splitting a raw text into characters Character-based vocabularies are slimmer There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. Limits: intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own in compared to any word Their sequences are translated into very large amounts of tokens to be processed by the model and this can have an impact on the size of the contants the model will carry around and it will reduce the size of the text we can use as input for a model which is often limited very long sequences less meaningful individual tokens","title":"Character-based"},{"location":"2_3Tokenizers/#subword-tokenization","text":"Splitting a raw text into subwords Finding a middle ground between word and character-based algorithms Subword-based tokenization lies between character and worf-based algorithm Frequently used words should not be split into smaller subwords Rare words should be decomposed into meaningful subwords Subwords help identify similar syntactic or semantic situations in text Subword tokenization algorithms can identify start of word tokens and which tokens complete start of words Most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. WordPiece: BERT, DistilBERT Unigram: XLNet, ALBERT Byte-Pair Encoding: GPT-2, RoBERTa These approaches help in reducing the vocabulary sizes by sharing information across different words having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words by recognizing similar tokens making them up.","title":"Subword tokenization"},{"location":"2_3Tokenizers/#tokenizer-pipeline","text":"A tokenizer takes texts as inputs and outputs numbers the associated model can make sense of from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Tokenization pipeline: from input text to a list of numbers: Raw text: Let's try to tokenize! Tokens: [let, ', s, try, to, token, ##ize, !] Special tokens: [[CLS],let,',s, try, to, token, ##ize,!,[SEP]] Input IDs: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] The first step of the pipeline is to split the text into tokens. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[let, ', s, try, to, token, ##ize, !] Albert tokenize will add a long underscore in front of all the tokens that added space before them which is a convention shared by all sentence-based tokenizers from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"albert-base-v1\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[_let, ', s, _try, _to, _to, ken, ize, !] The second step of the tokenization pipeline is to map those tokens to their respective input ids as defined by the vocabulary of the tokenizer. This is why we need to download the file when we instantiate a tokenizer with from_pretrained() method. We have to make sure we use the same mapping as when the model was pretrained. To do this, we use convert_tokens_to_ids() method: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( input_ids ) #[2292, 1005, 1055, 3046, 2000, 19204, 4697, 999] Lastly, the tokenizer adds special tokens the model expects by prepare_for_model() method: final_inputs = tokenizer . prepare_for_model ( input_ids ) print ( final_inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]","title":"Tokenizer Pipeline"},{"location":"2_3Tokenizers/#decoding","text":"The decode method allows us to check how the final output of the tokenizer translates back into text. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"[CLS] let's try to tokenize! [SEP]\" Roberta tokenizers uses special tokens as : & from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"roberta\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"<s> let's try to tokenize! </s>\" Hence, a tokenizer takes texts as inputs and outputs numbers the associated model can make sense of. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) # {'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}","title":"Decoding"},{"location":"2_4HandlingMultipleSequences/","text":"Handling multiple sequences Batching inputs together: Sentences we want to group inside a batch will often have different lengths from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] tokens = [ tokenizer . tokenize ( sentence ) for sentence in sentences ] ids = [ tokenizer . convert_tokens_to_ids ( token ) for token in tokens ] print ( ids [ 0 ]) print ( ids [ 1 ]) #[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] #[1045, 5223, 2023, 1012] You can't build a tensor with lists of different lengths because all arrays and tensors should be rectangular import torch ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 ]] input_ids = torch . tensor ( ids ) # ValueError: expected sequence of length 14 at dim 1 (got 4) Generally, we only truncate sentences when they are longer than the maximum length the model can handle Which is why we usually pad the smaller sentences to the length of the longest one! import torch ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] input_ids = torch . tensor ( ids ) input_ids from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer . pad_token_id # Applying padding here Now that we have padded our sentences we can make a batch with them But just passing this through a transformers model will not give the right results. from transformers import AutoModelForSequenceClassification ids1 = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ]] ) ids2 = torch . tensor ([[ 1045 , 5223 , 2023 , 1012 ]]) all_ids = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) print ( model ( ids1 ) . logits ) print ( model ( ids2 ) . logits ) print ( model ( all_ids ) . logits ) \"\"\" tensor([[-2.7276, 2.8789]], grad_fn=<AddmmBackward>) tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>) tensor([[-2.7276, 2.8789], [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>) \"\"\" This is because the attention layers use the padding tokens in the context they look at for each token in the sentence. Attention layers attend just the 4 tokens: [I, hate, this, !] Attention layers attend the 4 tokens and all padding tokens: [I, hate, this, !, [PAD], [PAD], [PAD], [PAD]] To tell the attention layers to ignore the padding tokens, we need to pass them an attention mask. all_ids = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) # adding attention by creating attention mask attention_mask = torch . tensor ( [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) Here, attention layers will ignore the tokens marked with 0 in the attention mask. With the proper attention mask, predictions are the same for a given sentence, with or without padding. model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) output1 = model ( ids1 ) output2 = model ( ids2 ) print ( output1 . logits ) print ( output2 . logits ) output = model ( all_ids , attention_mask = attention_mask ) print ( output . logits ) Using with padding=True, the tokenizer can directly prepare the inputs with padding and the proper attention mask: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] print ( tokenizer ( sentences , padding = True )) # {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], # 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}","title":"Handling Multiple Sequences"},{"location":"2_4HandlingMultipleSequences/#handling-multiple-sequences","text":"","title":"Handling multiple sequences"},{"location":"2_4HandlingMultipleSequences/#batching-inputs-together","text":"Sentences we want to group inside a batch will often have different lengths from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] tokens = [ tokenizer . tokenize ( sentence ) for sentence in sentences ] ids = [ tokenizer . convert_tokens_to_ids ( token ) for token in tokens ] print ( ids [ 0 ]) print ( ids [ 1 ]) #[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] #[1045, 5223, 2023, 1012] You can't build a tensor with lists of different lengths because all arrays and tensors should be rectangular import torch ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 ]] input_ids = torch . tensor ( ids ) # ValueError: expected sequence of length 14 at dim 1 (got 4) Generally, we only truncate sentences when they are longer than the maximum length the model can handle Which is why we usually pad the smaller sentences to the length of the longest one! import torch ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] input_ids = torch . tensor ( ids ) input_ids from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer . pad_token_id # Applying padding here Now that we have padded our sentences we can make a batch with them But just passing this through a transformers model will not give the right results. from transformers import AutoModelForSequenceClassification ids1 = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ]] ) ids2 = torch . tensor ([[ 1045 , 5223 , 2023 , 1012 ]]) all_ids = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) print ( model ( ids1 ) . logits ) print ( model ( ids2 ) . logits ) print ( model ( all_ids ) . logits ) \"\"\" tensor([[-2.7276, 2.8789]], grad_fn=<AddmmBackward>) tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>) tensor([[-2.7276, 2.8789], [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>) \"\"\" This is because the attention layers use the padding tokens in the context they look at for each token in the sentence. Attention layers attend just the 4 tokens: [I, hate, this, !] Attention layers attend the 4 tokens and all padding tokens: [I, hate, this, !, [PAD], [PAD], [PAD], [PAD]] To tell the attention layers to ignore the padding tokens, we need to pass them an attention mask. all_ids = torch . tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) # adding attention by creating attention mask attention_mask = torch . tensor ( [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) Here, attention layers will ignore the tokens marked with 0 in the attention mask. With the proper attention mask, predictions are the same for a given sentence, with or without padding. model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) output1 = model ( ids1 ) output2 = model ( ids2 ) print ( output1 . logits ) print ( output2 . logits ) output = model ( all_ids , attention_mask = attention_mask ) print ( output . logits ) Using with padding=True, the tokenizer can directly prepare the inputs with padding and the proper attention mask: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] print ( tokenizer ( sentences , padding = True )) # {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], # 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}","title":"Batching inputs together:"},{"location":"2_5PuttingAllTogether/","text":"We\u2019ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, attention masks. Tokenization When we call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) - Here, the model_inputs variable contains everything that\u2019s necessary for a model to operate well. tokenize a single sequence: # tokenize a single sequence sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) tokenize multiple sequences at a time: # tokenize multiple sequences at a time sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] model_inputs = tokenizer ( sequences ) It can pad according to several objectives: # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer ( sequences , padding = \"longest\" ) # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , padding = \"max_length\" ) # Will pad the sequences up to the specified max length model_inputs = tokenizer ( sequences , padding = \"max_length\" , max_length = 8 ) It can also truncate sequences: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , truncation = True ) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer ( sequences , max_length = 8 , truncation = True ) The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. Prompting the tokenizer to return tensors from the different frameworks \u2014 \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Returns PyTorch tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"pt\" ) # Returns TensorFlow tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"tf\" ) # Returns NumPy arrays model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"np\" ) Special tokens sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) print ( model_inputs [ \"input_ids\" ]) tokens = tokenizer . tokenize ( sequence ) ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( ids ) # decode the tokens print ( tokenizer . decode ( model_inputs [ \"input_ids\" ])) print ( tokenizer . decode ( ids )) # \"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\" # \"i've been waiting for a huggingface course my whole life.\" The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don\u2019t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. Wrapping up: From tokenizer to model one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API: import torch from transformers import AutoTokenizer , AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] tokens = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) output = model ( ** tokens )","title":"Putting All Together"},{"location":"2_5PuttingAllTogether/#tokenization","text":"When we call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) - Here, the model_inputs variable contains everything that\u2019s necessary for a model to operate well. tokenize a single sequence: # tokenize a single sequence sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) tokenize multiple sequences at a time: # tokenize multiple sequences at a time sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] model_inputs = tokenizer ( sequences ) It can pad according to several objectives: # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer ( sequences , padding = \"longest\" ) # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , padding = \"max_length\" ) # Will pad the sequences up to the specified max length model_inputs = tokenizer ( sequences , padding = \"max_length\" , max_length = 8 ) It can also truncate sequences: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , truncation = True ) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer ( sequences , max_length = 8 , truncation = True ) The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. Prompting the tokenizer to return tensors from the different frameworks \u2014 \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Returns PyTorch tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"pt\" ) # Returns TensorFlow tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"tf\" ) # Returns NumPy arrays model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"np\" )","title":"Tokenization"},{"location":"2_5PuttingAllTogether/#special-tokens","text":"sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) print ( model_inputs [ \"input_ids\" ]) tokens = tokenizer . tokenize ( sequence ) ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( ids ) # decode the tokens print ( tokenizer . decode ( model_inputs [ \"input_ids\" ])) print ( tokenizer . decode ( ids )) # \"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\" # \"i've been waiting for a huggingface course my whole life.\" The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don\u2019t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end.","title":"Special tokens"},{"location":"2_5PuttingAllTogether/#wrapping-up-from-tokenizer-to-model","text":"one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API: import torch from transformers import AutoTokenizer , AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] tokens = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) output = model ( ** tokens )","title":"Wrapping up: From tokenizer to model"},{"location":"3FineTuningPretrainedModel/","text":"Introduction We will learn: How to prepare a large dataset from the Hub How to use Trainer API to fine-tune a model How to use a custom training loop How to leverage the Huggingface Accelerate library to easily run that custom training loop on any distributed setup Learned: Learned about datasets in the Hub Learned how to load and preprocess datasets, including using dynamic padding and collators Implemented your own fine-tuning and evaluation of a model Implemented a lower-level training loop Used HuggingFace Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs","title":"Introduction"},{"location":"3FineTuningPretrainedModel/#introduction","text":"We will learn: How to prepare a large dataset from the Hub How to use Trainer API to fine-tune a model How to use a custom training loop How to leverage the Huggingface Accelerate library to easily run that custom training loop on any distributed setup Learned: Learned about datasets in the Hub Learned how to load and preprocess datasets, including using dynamic padding and collators Implemented your own fine-tuning and evaluation of a model Implemented a lower-level training loop Used HuggingFace Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs","title":"Introduction"},{"location":"3_1ProcessingData/","text":"Processing the data Train a sequence classifier on one batch in PyTorch: import torch from transformers import AdamW , AutoTokenizer , AutoModelForSequenceClassification # Same as before checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) # This is new batch [ \"labels\" ] = torch . tensor ([ 1 , 1 ]) optimizer = AdamW ( model . parameters ()) loss = model ( ** batch ) . loss loss . backward () optimizer . step () - Better accuracy required bigger dataset. - Dataset used below: MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper - The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Loading a dataset from the Hub Huggingface dataset library is a library that provides an api to quickly download many public's datasets and preprocess them. Huggingface dataset library allows you to easily download and cache datasets from its identifier on dataset hub. MRPC dataset is one of the 10 datasets composing the GLUE benchmark , which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) raw_datasets It returns a DatasetDict object which is a sort of dictionary containing each split(train,validation & test set) of the dataset. This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. We can access any split of dataset by its key, then any element by index. raw_datasets [ \"train\" ] - Each of the splits contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). We can access en element(s) by indexing: raw_datasets [ \"train\" ][ 6 ] raw_datasets [ \"train\" ][: 5 ] - We can also directly get a slice of our dataset. The features attributes gives us more information about each column. It gives corresponds between integer and names for tha labels. raw_datasets [ \"train\" ] . features { 'sentence1' : Value ( dtype = 'string' , id = None ), 'sentence2' : Value ( dtype = 'string' , id = None ), 'label' : ClassLabel ( num_classes = 2 , names = [ 'not_equivalent' , 'equivalent' ], names_file = None , id = None ), 'idx' : Value ( dtype = 'int32' , id = None )} - label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent. The map method allows you to apply a function over all the splits of a given dataset. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( example ): return tokenizer ( example [ \"sentence1\" ], example [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function ) print ( tokenized_datasets . column_names ) As long as the function returns a dictionary like object, the map() method will add new columns as needed or update existing ones. Result of tokenize_function(): ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids']. You can preprocess faster by using the option batched=True. The applied function will then receive multiple examples at each call. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) print ( tokenized_datasets . column_names ) We can also use multiprocessing with a map method. With just a few last tweaks, the dataset is then ready for training! We just remove the columns we don't need anymore with the remove column methods, rename label to labels since the model from transformers library expect that and set the output format to desired backend torch, tensorflow or numpy. If needed we can also generate a short sample of dataset using the select method. tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"tensorflow\" ) tokenized_datasets [ \"train\" ] small_train_dataset = tokenized_datasets [ \"train\" ] . select ( range ( 100 )) Preprocessing a dataset Preprocessing sentence pairs (PyTorch) We have seen before how to tokenize single sentences and batch them together. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) But text classification can also be applied on pairs of sentences. In a problem called Natural Language Inference(NLI), were we check whether a pair of sentences are logically related or not. In fact, the GLUE benchmark, 8 of the 10 tasks concern pair of sentences. Datasets with single sentences: COLA, SST-2 Datasets with pairs of sentences: MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI Models like BERT are pretrained to recognize relationships between 2 sentences. The tokenizers accept sentence pairs as well as single sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( \"My name is Pallavi Saxena.\" , \"I work at Avalara.\" ) It returns a new field called \"token_type_ids\" which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. The tokenizer adds special tokens for the corresponding model and prepares \"token_type_ids\" to indicate which part of the inputs correspond to which sentence. the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1. To process several pairs of sentences together, just pass the list of first sentences followed by the list of second sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True ) Tokenizers prepare the proper token type IDs and attention masks. Those inputs are then ready to go through a sequence classification model! from transformers import AutoModelForSequenceClassification , AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) batch = tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True , return_tensors = \"pt\" , ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** batch ) Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias'] This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Dynamic padding We need to pad sentences of different lengths to make batches. The first way to do this is to pad all the sentences in the whole datset to the maximum length in the dataset. Its downside is sentences with short sentences will have a lot of padding tokens, which will include more computations in the model which actually not needed. Another way is to pad the sentences at the batch creation, to the length of the longest sentence; a technique called dynamic padding. Pros: All the batches will have the smallest size possible. Con: dynamic shapes don't work well on the accelerators. All batches have different shapes slows down things on accelerators like TPUs. In practice, here is how we can preprocess the MRPC dataset with fixed padding: from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) We can then use our dataset in a standard PyTorch DataLoader. As expected, we get batches of fixed shapes from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break To apply dynamic padding, we postpone the padding in the preprocessing function. from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) # removed padding from here tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) Each batch then has a different size, but there is no needless padding. from torch.utils.data import DataLoader from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding ( tokenizer ) # applying padding her for dynamic padding train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True , collate_fn = data_collator ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break","title":"Processing Data"},{"location":"3_1ProcessingData/#processing-the-data","text":"Train a sequence classifier on one batch in PyTorch: import torch from transformers import AdamW , AutoTokenizer , AutoModelForSequenceClassification # Same as before checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) # This is new batch [ \"labels\" ] = torch . tensor ([ 1 , 1 ]) optimizer = AdamW ( model . parameters ()) loss = model ( ** batch ) . loss loss . backward () optimizer . step () - Better accuracy required bigger dataset. - Dataset used below: MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper - The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).","title":"Processing the data"},{"location":"3_1ProcessingData/#loading-a-dataset-from-the-hub","text":"Huggingface dataset library is a library that provides an api to quickly download many public's datasets and preprocess them. Huggingface dataset library allows you to easily download and cache datasets from its identifier on dataset hub. MRPC dataset is one of the 10 datasets composing the GLUE benchmark , which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) raw_datasets It returns a DatasetDict object which is a sort of dictionary containing each split(train,validation & test set) of the dataset. This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. We can access any split of dataset by its key, then any element by index. raw_datasets [ \"train\" ] - Each of the splits contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). We can access en element(s) by indexing: raw_datasets [ \"train\" ][ 6 ] raw_datasets [ \"train\" ][: 5 ] - We can also directly get a slice of our dataset. The features attributes gives us more information about each column. It gives corresponds between integer and names for tha labels. raw_datasets [ \"train\" ] . features { 'sentence1' : Value ( dtype = 'string' , id = None ), 'sentence2' : Value ( dtype = 'string' , id = None ), 'label' : ClassLabel ( num_classes = 2 , names = [ 'not_equivalent' , 'equivalent' ], names_file = None , id = None ), 'idx' : Value ( dtype = 'int32' , id = None )} - label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent. The map method allows you to apply a function over all the splits of a given dataset. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( example ): return tokenizer ( example [ \"sentence1\" ], example [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function ) print ( tokenized_datasets . column_names ) As long as the function returns a dictionary like object, the map() method will add new columns as needed or update existing ones. Result of tokenize_function(): ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids']. You can preprocess faster by using the option batched=True. The applied function will then receive multiple examples at each call. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) print ( tokenized_datasets . column_names ) We can also use multiprocessing with a map method. With just a few last tweaks, the dataset is then ready for training! We just remove the columns we don't need anymore with the remove column methods, rename label to labels since the model from transformers library expect that and set the output format to desired backend torch, tensorflow or numpy. If needed we can also generate a short sample of dataset using the select method. tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"tensorflow\" ) tokenized_datasets [ \"train\" ] small_train_dataset = tokenized_datasets [ \"train\" ] . select ( range ( 100 ))","title":"Loading a dataset from the Hub"},{"location":"3_1ProcessingData/#preprocessing-a-dataset","text":"","title":"Preprocessing a dataset"},{"location":"3_1ProcessingData/#preprocessing-sentence-pairs-pytorch","text":"We have seen before how to tokenize single sentences and batch them together. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"pt\" ) But text classification can also be applied on pairs of sentences. In a problem called Natural Language Inference(NLI), were we check whether a pair of sentences are logically related or not. In fact, the GLUE benchmark, 8 of the 10 tasks concern pair of sentences. Datasets with single sentences: COLA, SST-2 Datasets with pairs of sentences: MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI Models like BERT are pretrained to recognize relationships between 2 sentences. The tokenizers accept sentence pairs as well as single sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( \"My name is Pallavi Saxena.\" , \"I work at Avalara.\" ) It returns a new field called \"token_type_ids\" which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. The tokenizer adds special tokens for the corresponding model and prepares \"token_type_ids\" to indicate which part of the inputs correspond to which sentence. the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1. To process several pairs of sentences together, just pass the list of first sentences followed by the list of second sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True ) Tokenizers prepare the proper token type IDs and attention masks. Those inputs are then ready to go through a sequence classification model! from transformers import AutoModelForSequenceClassification , AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) batch = tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True , return_tensors = \"pt\" , ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** batch ) Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias'] This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.","title":"Preprocessing sentence pairs (PyTorch)"},{"location":"3_1ProcessingData/#dynamic-padding","text":"We need to pad sentences of different lengths to make batches. The first way to do this is to pad all the sentences in the whole datset to the maximum length in the dataset. Its downside is sentences with short sentences will have a lot of padding tokens, which will include more computations in the model which actually not needed. Another way is to pad the sentences at the batch creation, to the length of the longest sentence; a technique called dynamic padding. Pros: All the batches will have the smallest size possible. Con: dynamic shapes don't work well on the accelerators. All batches have different shapes slows down things on accelerators like TPUs. In practice, here is how we can preprocess the MRPC dataset with fixed padding: from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) We can then use our dataset in a standard PyTorch DataLoader. As expected, we get batches of fixed shapes from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break To apply dynamic padding, we postpone the padding in the preprocessing function. from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) # removed padding from here tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) Each batch then has a different size, but there is no needless padding. from torch.utils.data import DataLoader from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding ( tokenizer ) # applying padding her for dynamic padding train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True , collate_fn = data_collator ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break","title":"Dynamic padding"},{"location":"3_2ModelFineTuningWithTrainerAPI/","text":"The Trainer API: The transformers provide a Trainer API to easily train or fine-tune Transformer models on your dataset. The trainer class secure datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup(cpu, gpu) and can also compute the predictions on any datasets and if you provide metrics, it will evaluate your model on any dataset You can also involve the final data processing such as dynamic padding if you provide tokenizer or given data collator Preprocessing: Here is how we can easily preprocess the GLUE MRPC dataset using dynamic padding. from datasets import load_dataset from transformers import AutoTokenizer , DataCollatorWithPadding raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) data_collator = DataCollatorWithPadding ( tokenizer ) We do not apply padding during preprocessing as we will use dynamic padding with our \"DataCollatorWithPadding\" method. Note that we don't do the final steps of renaming, removing columns or set the format to torch tensors. As the Trainer will do all of these automatically for us by analyzing the model's signature. Model We also need a model and some training arguments brfore creating the Trainer. TrainingArguments class only takes a path to a folder where results and checkpoints will be saved. from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) from transformers import TrainingArguments training_args = TrainingArguments ( \"test-trainer\" ) You can also customize all the other parameters that your trainer will use- learning rate, num of training epochs, etc. from transformers import TrainingArguments training_args = TrainingArguments ( \"test-trainer\" , per_device_train_batch_size = 16 , per_device_eval_batch_size = 16 , num_train_epochs = 5 , learning_rate = 2e-5 , weight_decay = 0.01 , ) Training: We can then pass everything to the Trainer class and start training. from transformers import Trainer trainer = Trainer ( model , training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], data_collator = data_collator , tokenizer = tokenizer , ) trainer . train () The result will display training loss which doesn't really tell anything about how well/worse your model is performing. This is because we didn't specify any metric for the evaluation. Predictions: The predict method allows us to get the predictions of our model on a whole dataset. We can then use those predictions to compute metrics. predictions = trainer . predict ( tokenized_datasets [ \"validation\" ]) print ( predictions . predictions . shape , predictions . label_ids . shape ) # (408, 2) (408,) It returns a named tuple with 3 fields: predictions: contains the model predictions label_ids: contains the labels if your dataset have them metrics: it is empty here (we are trying to do that). Predictions are the logits of the models for all the sentences in the datset of shape: (408, 2) To match them with our labels, we need to take the maximum logits for each predictions to know which of the 2 classes was predicted. We do this with argmax() Then can use the matrix from dataset libray with load_metric() and it returns the evaluation metric used for the dataset. import numpy as np from datasets import load_metric metric = load_metric ( \"glue\" , \"mrpc\" ) preds = np . argmax ( predictions . predictions , axis =- 1 ) metric . compute ( predictions = preds , references = predictions . label_ids ) # {'accuracy': 0.8627450980392157, 'f1': 0.9050847457627118} We can see our model did learn something as it is 86.5% accurate. To monitor evaluation metrics during training, we need to define a compute_metrics() (as in above steps) and pass it to the trainer. It takes the named tuple with predictions and labels and must return a dictionary of the metrics we want to keep track of. By passing the epoch evaluation strategy to our training arguments, we tell the trainer to evaluate at the end of every epoch. metric = load_metric ( \"glue\" , \"mrpc\" ) def compute_metrics ( eval_preds ): logits , labels = eval_preds predictions = np . argmax ( logits , axis =- 1 ) return metric . compute ( predictions = predictions , references = labels ) training_args = TrainingArguments ( \"test-trainer\" , evaluation_strategy = \"epoch\" ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) trainer = Trainer ( model , training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], data_collator = data_collator , tokenizer = tokenizer , compute_metrics = compute_metrics ) trainer . train ()","title":"Fine-Tuning A Model with Trainer API"},{"location":"3_2ModelFineTuningWithTrainerAPI/#the-trainer-api","text":"The transformers provide a Trainer API to easily train or fine-tune Transformer models on your dataset. The trainer class secure datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup(cpu, gpu) and can also compute the predictions on any datasets and if you provide metrics, it will evaluate your model on any dataset You can also involve the final data processing such as dynamic padding if you provide tokenizer or given data collator","title":"The Trainer API:"},{"location":"3_2ModelFineTuningWithTrainerAPI/#preprocessing","text":"Here is how we can easily preprocess the GLUE MRPC dataset using dynamic padding. from datasets import load_dataset from transformers import AutoTokenizer , DataCollatorWithPadding raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) data_collator = DataCollatorWithPadding ( tokenizer ) We do not apply padding during preprocessing as we will use dynamic padding with our \"DataCollatorWithPadding\" method. Note that we don't do the final steps of renaming, removing columns or set the format to torch tensors. As the Trainer will do all of these automatically for us by analyzing the model's signature.","title":"Preprocessing:"},{"location":"3_2ModelFineTuningWithTrainerAPI/#model","text":"We also need a model and some training arguments brfore creating the Trainer. TrainingArguments class only takes a path to a folder where results and checkpoints will be saved. from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) from transformers import TrainingArguments training_args = TrainingArguments ( \"test-trainer\" ) You can also customize all the other parameters that your trainer will use- learning rate, num of training epochs, etc. from transformers import TrainingArguments training_args = TrainingArguments ( \"test-trainer\" , per_device_train_batch_size = 16 , per_device_eval_batch_size = 16 , num_train_epochs = 5 , learning_rate = 2e-5 , weight_decay = 0.01 , )","title":"Model"},{"location":"3_2ModelFineTuningWithTrainerAPI/#training","text":"We can then pass everything to the Trainer class and start training. from transformers import Trainer trainer = Trainer ( model , training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], data_collator = data_collator , tokenizer = tokenizer , ) trainer . train () The result will display training loss which doesn't really tell anything about how well/worse your model is performing. This is because we didn't specify any metric for the evaluation.","title":"Training:"},{"location":"3_2ModelFineTuningWithTrainerAPI/#predictions","text":"The predict method allows us to get the predictions of our model on a whole dataset. We can then use those predictions to compute metrics. predictions = trainer . predict ( tokenized_datasets [ \"validation\" ]) print ( predictions . predictions . shape , predictions . label_ids . shape ) # (408, 2) (408,) It returns a named tuple with 3 fields: predictions: contains the model predictions label_ids: contains the labels if your dataset have them metrics: it is empty here (we are trying to do that). Predictions are the logits of the models for all the sentences in the datset of shape: (408, 2) To match them with our labels, we need to take the maximum logits for each predictions to know which of the 2 classes was predicted. We do this with argmax() Then can use the matrix from dataset libray with load_metric() and it returns the evaluation metric used for the dataset. import numpy as np from datasets import load_metric metric = load_metric ( \"glue\" , \"mrpc\" ) preds = np . argmax ( predictions . predictions , axis =- 1 ) metric . compute ( predictions = preds , references = predictions . label_ids ) # {'accuracy': 0.8627450980392157, 'f1': 0.9050847457627118} We can see our model did learn something as it is 86.5% accurate. To monitor evaluation metrics during training, we need to define a compute_metrics() (as in above steps) and pass it to the trainer. It takes the named tuple with predictions and labels and must return a dictionary of the metrics we want to keep track of. By passing the epoch evaluation strategy to our training arguments, we tell the trainer to evaluate at the end of every epoch. metric = load_metric ( \"glue\" , \"mrpc\" ) def compute_metrics ( eval_preds ): logits , labels = eval_preds predictions = np . argmax ( logits , axis =- 1 ) return metric . compute ( predictions = predictions , references = labels ) training_args = TrainingArguments ( \"test-trainer\" , evaluation_strategy = \"epoch\" ) model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) trainer = Trainer ( model , training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], data_collator = data_collator , tokenizer = tokenizer , compute_metrics = compute_metrics ) trainer . train ()","title":"Predictions:"},{"location":"3_3FullTraining/","text":"Write your training loop in PyTorch Preprocessing Here is how we can easily preprocess the GLUE MRPC dataset using dynamic padding. from datasets import load_dataset from transformers import AutoTokenizer , DataCollatorWithPadding raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"sentence1\" , \"sentence2\" , \"idx\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets . set_format ( \"torch\" ) data_collator = DataCollatorWithPadding ( tokenizer ) Once our data is preprocessed, we just have to create our DataLoaders which will be responsible to convert our dataset into batches. from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], shuffle = True , batch_size = 8 , collate_fn = data_collator ) eval_dataloader = DataLoader ( tokenized_datasets [ \"validation\" ], batch_size = 8 , collate_fn = data_collator ) To check everything works as intended, we try to grab a batch of data and inspect it. for batch in train_dataloader : break print ({ k : v . shape for k , v in batch . items ()}) # {'attention_mask': torch.Size([8, 63]), 'input_ids': torch.Size([8, 63]), 'labels': torch.Size([8]), 'token_type_ids': torch.Size([8, 63])} Like dataset element it is dictionary, but this time these values are not a single list of integers but a tons of batches of shape batch size by sequence length. Model The next step is to create our model and send our training data into model. We will use from_pretrained() method and adjust the number of labels to the number of classes we have in our dataset(here, 2): from transformers import AutoModelForSequenceClassification checkpoint = \"bert-base-cased\" model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) To be sure everything is going well, we pass the batch required to our model and check there is no error outputs = model ( ** batch ) print ( outputs . loss , outputs . logits . shape ) # tensor(0.7512, grad_fn=<NllLossBackward>) torch.Size([8, 2]) Training: These labels are provided, so the models of the transformers library always returns the loss directly. Will use loss.backward() to compute the gradients. Then we will use the optimizer to do the training step. The optimizer will be responsible for doing the training updates to the model weights. from transformers import AdamW optimizer = AdamW ( model . parameters (), lr = 5e-5 ) loss = outputs . loss loss . backward () optimizer . step () # Don't forget to zero your gradients once your optimizer step is done! optimizer . zero_grad () We will add 2 more things to make it as good as it can be. The first one is lr schedular. The learning rate schedular will update the optimizer's learning rate at each step. from transformers import get_scheduler num_epochs = 3 num_training_steps = num_epochs * len ( train_dataloader ) lr_scheduler = get_scheduler ( \"linear\" , optimizer = optimizer , num_warmup_steps = 0 , num_training_steps = num_training_steps ) You can use any Pytorch leaning rate schedular in place of it. We can make training faster by using GPU instead of CPU. import torch device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) model . to ( device ) print ( device ) Putting everything together, here is what the training loop looks like. optimizer = AdamW ( model . parameters (), lr = 5e-5 ) from tqdm.auto import tqdm progress_bar = tqdm ( range ( num_training_steps )) model . train () for epoch in range ( num_epochs ): for batch in train_dataloader : batch = { k : v . to ( device ) for k , v in batch . items ()} outputs = model ( ** batch ) loss = outputs . loss loss . backward () optimizer . step () lr_scheduler . step () optimizer . zero_grad () progress_bar . update ( 1 ) Evaluation Evaluation can then be done like this with a datasets metric. First we put our model in the evaluation mode to deactivate layers like dropout, then go through all the evaluation that are needed. Model provides logits and we need to provide argmax function to convert them into predictions from datasets import load_metric metric = load_metric ( \"glue\" , \"mrpc\" ) model . eval () for batch in eval_dataloader : batch = { k : v . to ( device ) for k , v in batch . items ()} with torch . no_grad (): outputs = model ( ** batch ) logits = outputs . logits predictions = torch . argmax ( logits , dim =- 1 ) metric . add_batch ( predictions = predictions , references = batch [ \"labels\" ]) metric . compute () metric.add_batch() to send to the predictions.","title":"Full Training"},{"location":"3_3FullTraining/#write-your-training-loop-in-pytorch","text":"","title":"Write your training loop in PyTorch"},{"location":"3_3FullTraining/#preprocessing","text":"Here is how we can easily preprocess the GLUE MRPC dataset using dynamic padding. from datasets import load_dataset from transformers import AutoTokenizer , DataCollatorWithPadding raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"sentence1\" , \"sentence2\" , \"idx\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets . set_format ( \"torch\" ) data_collator = DataCollatorWithPadding ( tokenizer ) Once our data is preprocessed, we just have to create our DataLoaders which will be responsible to convert our dataset into batches. from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], shuffle = True , batch_size = 8 , collate_fn = data_collator ) eval_dataloader = DataLoader ( tokenized_datasets [ \"validation\" ], batch_size = 8 , collate_fn = data_collator ) To check everything works as intended, we try to grab a batch of data and inspect it. for batch in train_dataloader : break print ({ k : v . shape for k , v in batch . items ()}) # {'attention_mask': torch.Size([8, 63]), 'input_ids': torch.Size([8, 63]), 'labels': torch.Size([8]), 'token_type_ids': torch.Size([8, 63])} Like dataset element it is dictionary, but this time these values are not a single list of integers but a tons of batches of shape batch size by sequence length.","title":"Preprocessing"},{"location":"3_3FullTraining/#model","text":"The next step is to create our model and send our training data into model. We will use from_pretrained() method and adjust the number of labels to the number of classes we have in our dataset(here, 2): from transformers import AutoModelForSequenceClassification checkpoint = \"bert-base-cased\" model = AutoModelForSequenceClassification . from_pretrained ( checkpoint , num_labels = 2 ) To be sure everything is going well, we pass the batch required to our model and check there is no error outputs = model ( ** batch ) print ( outputs . loss , outputs . logits . shape ) # tensor(0.7512, grad_fn=<NllLossBackward>) torch.Size([8, 2])","title":"Model"},{"location":"3_3FullTraining/#training","text":"These labels are provided, so the models of the transformers library always returns the loss directly. Will use loss.backward() to compute the gradients. Then we will use the optimizer to do the training step. The optimizer will be responsible for doing the training updates to the model weights. from transformers import AdamW optimizer = AdamW ( model . parameters (), lr = 5e-5 ) loss = outputs . loss loss . backward () optimizer . step () # Don't forget to zero your gradients once your optimizer step is done! optimizer . zero_grad () We will add 2 more things to make it as good as it can be. The first one is lr schedular. The learning rate schedular will update the optimizer's learning rate at each step. from transformers import get_scheduler num_epochs = 3 num_training_steps = num_epochs * len ( train_dataloader ) lr_scheduler = get_scheduler ( \"linear\" , optimizer = optimizer , num_warmup_steps = 0 , num_training_steps = num_training_steps ) You can use any Pytorch leaning rate schedular in place of it. We can make training faster by using GPU instead of CPU. import torch device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) model . to ( device ) print ( device ) Putting everything together, here is what the training loop looks like. optimizer = AdamW ( model . parameters (), lr = 5e-5 ) from tqdm.auto import tqdm progress_bar = tqdm ( range ( num_training_steps )) model . train () for epoch in range ( num_epochs ): for batch in train_dataloader : batch = { k : v . to ( device ) for k , v in batch . items ()} outputs = model ( ** batch ) loss = outputs . loss loss . backward () optimizer . step () lr_scheduler . step () optimizer . zero_grad () progress_bar . update ( 1 )","title":"Training:"},{"location":"3_3FullTraining/#evaluation","text":"Evaluation can then be done like this with a datasets metric. First we put our model in the evaluation mode to deactivate layers like dropout, then go through all the evaluation that are needed. Model provides logits and we need to provide argmax function to convert them into predictions from datasets import load_metric metric = load_metric ( \"glue\" , \"mrpc\" ) model . eval () for batch in eval_dataloader : batch = { k : v . to ( device ) for k , v in batch . items ()} with torch . no_grad (): outputs = model ( ** batch ) logits = outputs . logits predictions = torch . argmax ( logits , dim =- 1 ) metric . add_batch ( predictions = predictions , references = batch [ \"labels\" ]) metric . compute () metric.add_batch() to send to the predictions.","title":"Evaluation"},{"location":"4HuggingFaceHub/","text":"The Hugging Face Hub We will focus on the models in this section, and take a look at the datasets in the next section. Navigating the Model Hub Huggingface landing page: https://huggingface.co/ To access models: https://huggingface.co/models Left side has different categories: Tasks, framework, language, datasets, etc Selecting model shows its model card. The model card contains information about the model, its description, intended use, limitations and biases. It can also show code snippets on how to use the model as well as relevant information, training procedure, data processing, evaluation results or copyrights. On right of the model card is the inference api, it can be used to play with the model directly. At the top of the model card lies the model tags. These includes the model task as well as any other tag that is relevant to the categories on the left of the model screen. The files and vsersions tab displays the architecture of the repository of that model. Here, we can see all the files that define this model. You will see all usual features of a git repository, branches available, commit history as well as commit diff. 3 different buttons are available at the top of the model card: Use Accelerated Inference: the first one shows how to use the inference api programmatically Use in SageMaker: the second one shows how to train this model in SageMaker Use in Transformers: the last one shows how to load that model within the appropriate library","title":"The Hugging Face Hub"},{"location":"4HuggingFaceHub/#the-hugging-face-hub","text":"We will focus on the models in this section, and take a look at the datasets in the next section.","title":"The Hugging Face Hub"},{"location":"4HuggingFaceHub/#navigating-the-model-hub","text":"Huggingface landing page: https://huggingface.co/ To access models: https://huggingface.co/models Left side has different categories: Tasks, framework, language, datasets, etc Selecting model shows its model card. The model card contains information about the model, its description, intended use, limitations and biases. It can also show code snippets on how to use the model as well as relevant information, training procedure, data processing, evaluation results or copyrights. On right of the model card is the inference api, it can be used to play with the model directly. At the top of the model card lies the model tags. These includes the model task as well as any other tag that is relevant to the categories on the left of the model screen. The files and vsersions tab displays the architecture of the repository of that model. Here, we can see all the files that define this model. You will see all usual features of a git repository, branches available, commit history as well as commit diff. 3 different buttons are available at the top of the model card: Use Accelerated Inference: the first one shows how to use the inference api programmatically Use in SageMaker: the second one shows how to train this model in SageMaker Use in Transformers: the last one shows how to load that model within the appropriate library","title":"Navigating the Model Hub"},{"location":"4_1UsingPretrainedModels/","text":"Using pretrained models The Model Hub makes selecting the appropriate model simple. how to use one of these models, and how to contribute back to the community Let\u2019s say we\u2019re looking for a French-based model that can perform mask filling. Go to https://huggingface.co/models Select task category: Fill-Mask Select languages: French Click on a model(eg, camembert-base) to go to its model card. we can instantiate it using the pipeline() function: from transformers import pipeline camembert_fill_mask = pipeline ( \"fill-mask\" , model = \"camembert-base\" ) results = camembert_fill_mask ( \"Le camembert est <mask> :)\" ) # [ # {'sequence': 'Le camembert est d\u00e9licieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'd\u00e9licieux'}, # {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'}, # {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'}, # {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'}, # {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'} # ] loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it\u2019s going to be used for. For example, here we are loading the camembert-base checkpoint in the fill-mask pipeline, which is completely fine. But if we were to load this checkpoint in the text-classification pipeline, the results would not make any sense because the head of camembert-base is not suitable for this task! use the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints: You can also instantiate the checkpoint using the model architecture directly: from transformers import CamembertTokenizer , CamembertForMaskedLM tokenizer = CamembertTokenizer . from_pretrained ( \"camembert-base\" ) model = CamembertForMaskedLM . from_pretrained ( \"camembert-base\" ) However, we recommend using the Auto classes instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the Auto classes makes switching checkpoints simple: from transformers import AutoTokenizer , AutoModelForMaskedLM tokenizer = AutoTokenizer . from_pretrained ( \"camembert-base\" ) model = AutoModelForMaskedLM . from_pretrained ( \"camembert-base\" ) When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.","title":"Using Pretrained Models"},{"location":"4_1UsingPretrainedModels/#using-pretrained-models","text":"The Model Hub makes selecting the appropriate model simple. how to use one of these models, and how to contribute back to the community Let\u2019s say we\u2019re looking for a French-based model that can perform mask filling. Go to https://huggingface.co/models Select task category: Fill-Mask Select languages: French Click on a model(eg, camembert-base) to go to its model card. we can instantiate it using the pipeline() function: from transformers import pipeline camembert_fill_mask = pipeline ( \"fill-mask\" , model = \"camembert-base\" ) results = camembert_fill_mask ( \"Le camembert est <mask> :)\" ) # [ # {'sequence': 'Le camembert est d\u00e9licieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'd\u00e9licieux'}, # {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'}, # {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'}, # {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'}, # {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'} # ] loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it\u2019s going to be used for. For example, here we are loading the camembert-base checkpoint in the fill-mask pipeline, which is completely fine. But if we were to load this checkpoint in the text-classification pipeline, the results would not make any sense because the head of camembert-base is not suitable for this task! use the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints: You can also instantiate the checkpoint using the model architecture directly: from transformers import CamembertTokenizer , CamembertForMaskedLM tokenizer = CamembertTokenizer . from_pretrained ( \"camembert-base\" ) model = CamembertForMaskedLM . from_pretrained ( \"camembert-base\" ) However, we recommend using the Auto classes instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the Auto classes makes switching checkpoints simple: from transformers import AutoTokenizer , AutoModelForMaskedLM tokenizer = AutoTokenizer . from_pretrained ( \"camembert-base\" ) model = AutoModelForMaskedLM . from_pretrained ( \"camembert-base\" ) When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.","title":"Using pretrained models"},{"location":"4_2SharingPretrainedModels/","text":"Sharing pretrained models Below are the easiest ways to share pretrained models to the HuggingFace Hub. There are three ways to go about creating new model repositories: Using the push_to_hub API Using the huggingface_hub Python library Using the web interface Once you\u2019ve created a repository, you can upload files to it via git and git-lfs. Managing a repo on the Model Hub In order to handle a repository, you should first have a huggingface account: https://huggingface.co/join Once you are logged in, you can create a new repository by clicking on the new model option: https://huggingface.co/new Owner: pallavi176 # your namespace or your organization namespace Model name: dummy_model3 License: mit public (recommended free option) Click on create model Files and versions tab is kind of git repo (version contol) Adding files to the repository In Files and versions tab, files can be added through the web interface through add file button. Click Add file -> then Create a new file files: next_file.txt (can be of format txt, json, etc) content: new file (can add content to the file) Can add files using huggingface_hub library and through command line Upload files using huggingface_hub library login to your account: https://huggingface.co/pallavi176 ! pip install huggingface_hub from huggingface_hub import notebook_login # Login with huggingface write access token notebook_login () Upload file using upload_file() method: from huggingface_hub import upload_file # upload_file(\"Current loaction of the file\", 'path of the file in repo', 'id of the repo we are pushing') upload_file ( \"path_to_file\" , 'path_in__file in _repo' , '<namespace>/<repo_id>' ) - Additional parameters: - token: if you would like to specify a different token than the one saved in your cache with your login - repo_type: if you would loke to push to a 'dataset' or a 'space' Upload Readme file: with open ( \"/tmp/README.md\" , \"w+\" ) as f : f . write ( \"# My dummy model\" ) upload_file ( path_or_fileobj = \"/tmp/README.md\" , path_in_repo = \"README.md\" , repo_id = \"pallavi176/dummy_model3\" ) delete_file() method to delete the file from repo from huggingface_hub import delete_file delete_file ( path_in_repo = \"README.md\" , repo_id = \"pallavi176/dummy-model2\" ) This approach using only these 2 methods is super simple. It doesn't need git or git lfs installed Limitation: The maximum file size that can be uploaded is limited to GB Uploading using repository utility This class is a wrapper over git and git lfs methods which abstracts most of the complexity and offers a flexible api to manage your online repositories from huggingface_hub import Repository repo = Repository ( \"local-folder\" , clone_from = \"pallavi176/dummy_model3\" ) Cloned from \"pallavi176/dummy_model3\" huggingface repository to local directory: \"local-folder\" Upload a trained model from local: from transformer import AutoModelForSequenceClassification , AutoTokenizer model = AutoModelForSequenceClassification . from_pretrained ( \"/tmp/cool-model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"/tmp/cool-model\" ) repo . git_pull () Save the model & tokenizer files inside that folder model . save_pretrained ( repo . local_dir ) tokenizer . save_pretrained ( repo . local_dir ) We will start the file by adding git add method: repo . git_add () repo . git_commit ( \"Added model and tokenizer\" ) repo . git_push () repo . git_tag () The Push to Hub API (PyTorch) Login to huggingface using token id: from huggingface_hub import notebook_login # Login with huggingface write access token notebook_login () Launch fine tuning of bert model using gule cola dataset: from datasets import load_dataset , load_metric raw_datasets = load_dataset ( \"glue\" , \"cola\" ) from transformers import AutoTokenizer model_checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) def preprocess_function ( examples ): return tokenizer ( examples [ \"sentence\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( preprocess_function , batched = True ) from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification . from_pretrained ( model_checkpoint ) import numpy as np from datasets import load_metric metric = load_metric ( \"glue\" , \"cola\" ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) return metric . compute ( predictions = predictions , references = labels ) Push to hub by setting parameter in the training argument: push_to_hub=True from transformers import TrainingArguments args = TrainingArguments ( \"bert-fine-tuned-cola\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , num_train_epochs = 3 , weight_decay = 0.01 , push_to_hub = True , ) This will automatically upload your model to the app each time it is saved, so every epoch in our case We can choose which model_id to push to using argument hub_model_id=\"other name\" from transformers import TrainingArguments args = TrainingArguments ( \"bert-fine-tuned-cola\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , num_train_epochs = 3 , weight_decay = 0.01 , push_to_hub = True , hub_model_id = \"other name\" ) We can launch training and it will upload at every epoch as mentioned in the training argument from transformers import Trainer trainer = Trainer ( model , args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], compute_metrics = compute_metrics , tokenizer = tokenizer , ) trainer . train () ) trainer . push_to_hub ( \"End of training\" ) Pusing components individually If you are not using Trainer API to train your model, you can use push_to_hub() on model & tokenizer directly repo_name = \"bert-fine-tuned-cola\" model . push_to_hub ( repo_name ) tokenizer . push_to_hub ( repo_name ) To fix the existing labels on the hub: label_names = raw_datasets [ \"train\" ] . features [ \"label\" ] . names model . config . id2label = { str ( i ): lbl for i , lbl in enumerate ( label_names )} model . config . label2id = { lbl : str ( i ) for i , lbl in enumerate ( label_names )} repo_name = \"bert-fine-tuned-cola\" model . config . push_to_hub ( repo_name ) Use uploaded model from transformers import pipeline classifier = pipeline ( \"text-classification\" , model = \"pallavi176/bert-fine-tuned-cola\" ) classifier ( \"This is incorrect sentence.\" )","title":"Sharing Pretrained Models"},{"location":"4_2SharingPretrainedModels/#sharing-pretrained-models","text":"Below are the easiest ways to share pretrained models to the HuggingFace Hub. There are three ways to go about creating new model repositories: Using the push_to_hub API Using the huggingface_hub Python library Using the web interface Once you\u2019ve created a repository, you can upload files to it via git and git-lfs.","title":"Sharing pretrained models"},{"location":"4_2SharingPretrainedModels/#managing-a-repo-on-the-model-hub","text":"In order to handle a repository, you should first have a huggingface account: https://huggingface.co/join Once you are logged in, you can create a new repository by clicking on the new model option: https://huggingface.co/new Owner: pallavi176 # your namespace or your organization namespace Model name: dummy_model3 License: mit public (recommended free option) Click on create model Files and versions tab is kind of git repo (version contol)","title":"Managing a repo on the Model Hub"},{"location":"4_2SharingPretrainedModels/#adding-files-to-the-repository","text":"In Files and versions tab, files can be added through the web interface through add file button. Click Add file -> then Create a new file files: next_file.txt (can be of format txt, json, etc) content: new file (can add content to the file) Can add files using huggingface_hub library and through command line","title":"Adding files to the repository"},{"location":"4_2SharingPretrainedModels/#upload-files-using-huggingface_hub-library","text":"login to your account: https://huggingface.co/pallavi176 ! pip install huggingface_hub from huggingface_hub import notebook_login # Login with huggingface write access token notebook_login () Upload file using upload_file() method: from huggingface_hub import upload_file # upload_file(\"Current loaction of the file\", 'path of the file in repo', 'id of the repo we are pushing') upload_file ( \"path_to_file\" , 'path_in__file in _repo' , '<namespace>/<repo_id>' ) - Additional parameters: - token: if you would like to specify a different token than the one saved in your cache with your login - repo_type: if you would loke to push to a 'dataset' or a 'space' Upload Readme file: with open ( \"/tmp/README.md\" , \"w+\" ) as f : f . write ( \"# My dummy model\" ) upload_file ( path_or_fileobj = \"/tmp/README.md\" , path_in_repo = \"README.md\" , repo_id = \"pallavi176/dummy_model3\" )","title":"Upload files using huggingface_hub library"},{"location":"4_2SharingPretrainedModels/#delete_file-method-to-delete-the-file-from-repo","text":"from huggingface_hub import delete_file delete_file ( path_in_repo = \"README.md\" , repo_id = \"pallavi176/dummy-model2\" ) This approach using only these 2 methods is super simple. It doesn't need git or git lfs installed Limitation: The maximum file size that can be uploaded is limited to GB","title":"delete_file() method to delete the file from repo"},{"location":"4_2SharingPretrainedModels/#uploading-using-repository-utility","text":"This class is a wrapper over git and git lfs methods which abstracts most of the complexity and offers a flexible api to manage your online repositories from huggingface_hub import Repository repo = Repository ( \"local-folder\" , clone_from = \"pallavi176/dummy_model3\" ) Cloned from \"pallavi176/dummy_model3\" huggingface repository to local directory: \"local-folder\" Upload a trained model from local: from transformer import AutoModelForSequenceClassification , AutoTokenizer model = AutoModelForSequenceClassification . from_pretrained ( \"/tmp/cool-model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"/tmp/cool-model\" ) repo . git_pull () Save the model & tokenizer files inside that folder model . save_pretrained ( repo . local_dir ) tokenizer . save_pretrained ( repo . local_dir ) We will start the file by adding git add method: repo . git_add () repo . git_commit ( \"Added model and tokenizer\" ) repo . git_push () repo . git_tag ()","title":"Uploading using repository utility"},{"location":"4_2SharingPretrainedModels/#the-push-to-hub-api-pytorch","text":"Login to huggingface using token id: from huggingface_hub import notebook_login # Login with huggingface write access token notebook_login () Launch fine tuning of bert model using gule cola dataset: from datasets import load_dataset , load_metric raw_datasets = load_dataset ( \"glue\" , \"cola\" ) from transformers import AutoTokenizer model_checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) def preprocess_function ( examples ): return tokenizer ( examples [ \"sentence\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( preprocess_function , batched = True ) from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification . from_pretrained ( model_checkpoint ) import numpy as np from datasets import load_metric metric = load_metric ( \"glue\" , \"cola\" ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) return metric . compute ( predictions = predictions , references = labels ) Push to hub by setting parameter in the training argument: push_to_hub=True from transformers import TrainingArguments args = TrainingArguments ( \"bert-fine-tuned-cola\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , num_train_epochs = 3 , weight_decay = 0.01 , push_to_hub = True , ) This will automatically upload your model to the app each time it is saved, so every epoch in our case We can choose which model_id to push to using argument hub_model_id=\"other name\" from transformers import TrainingArguments args = TrainingArguments ( \"bert-fine-tuned-cola\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , num_train_epochs = 3 , weight_decay = 0.01 , push_to_hub = True , hub_model_id = \"other name\" ) We can launch training and it will upload at every epoch as mentioned in the training argument from transformers import Trainer trainer = Trainer ( model , args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], compute_metrics = compute_metrics , tokenizer = tokenizer , ) trainer . train () ) trainer . push_to_hub ( \"End of training\" )","title":"The Push to Hub API (PyTorch)"},{"location":"4_2SharingPretrainedModels/#pusing-components-individually","text":"If you are not using Trainer API to train your model, you can use push_to_hub() on model & tokenizer directly repo_name = \"bert-fine-tuned-cola\" model . push_to_hub ( repo_name ) tokenizer . push_to_hub ( repo_name )","title":"Pusing components individually"},{"location":"4_2SharingPretrainedModels/#to-fix-the-existing-labels-on-the-hub","text":"label_names = raw_datasets [ \"train\" ] . features [ \"label\" ] . names model . config . id2label = { str ( i ): lbl for i , lbl in enumerate ( label_names )} model . config . label2id = { lbl : str ( i ) for i , lbl in enumerate ( label_names )} repo_name = \"bert-fine-tuned-cola\" model . config . push_to_hub ( repo_name )","title":"To fix the existing labels on the hub:"},{"location":"4_2SharingPretrainedModels/#use-uploaded-model","text":"from transformers import pipeline classifier = pipeline ( \"text-classification\" , model = \"pallavi176/bert-fine-tuned-cola\" ) classifier ( \"This is incorrect sentence.\" )","title":"Use uploaded model"},{"location":"4_3BuildingModelCard/","text":"Building a Model Card It is the central definition of the model, ensuring reusability by fellow community members and reproducibility of results, and providing a platform on which other members may build their artifacts. Creating the model card is done through the README.md file, which is a Markdown file. The model card usually starts with a very brief, high-level overview of what the model is for, followed by additional details in the following sections: Model description Intended uses & limitations How to use Limitations and bias Training data Training procedure Evaluation results Model description The model description provides basic details about the model. This includes the architecture, version, if it was introduced in a paper, if an original implementation is available, the author, and general information about the model. Any copyright should be attributed here. General information about training procedures, parameters, and important disclaimers can also be mentioned in this section. Intended uses & limitations Here you describe the use cases the model is intended for, including the languages, fields, and domains where it can be applied. This section of the model card can also document areas that are known to be out of scope for the model, or where it is likely to perform suboptimally. How to use This section should include some examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes, and any other code you think might be helpful. Training data This part should indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome. Training procedure In this section you should describe all the relevant aspects of training that are useful from a reproducibility perspective. This includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, the batch size, the learning rate, and so on. Variable and metrics Here you should describe the metrics you use for evaluation, and the different factors you are mesuring. Mentioning which metric(s) were used, on which dataset and which dataset split, makes it easy to compare you model\u2019s performance compared to that of other models. These should be informed by the previous sections, such as the intended users and use cases. Evaluation results Finally, provide an indication of how well the model performs on the evaluation dataset. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses. Model cards are not a requirement when publishing models, and you don\u2019t need to include all of the sections described above when you make one. However, explicit documentation of the model can only benefit future users Model card metadata --- language: fr license: mit datasets: - oscar --- This metadata is parsed by the Hugging Face Hub, which then identifies this model as being a French model, with an MIT license, trained on the Oscar dataset.","title":"Building a Model Card"},{"location":"4_3BuildingModelCard/#building-a-model-card","text":"It is the central definition of the model, ensuring reusability by fellow community members and reproducibility of results, and providing a platform on which other members may build their artifacts. Creating the model card is done through the README.md file, which is a Markdown file. The model card usually starts with a very brief, high-level overview of what the model is for, followed by additional details in the following sections: Model description Intended uses & limitations How to use Limitations and bias Training data Training procedure Evaluation results","title":"Building a Model Card"},{"location":"4_3BuildingModelCard/#model-description","text":"The model description provides basic details about the model. This includes the architecture, version, if it was introduced in a paper, if an original implementation is available, the author, and general information about the model. Any copyright should be attributed here. General information about training procedures, parameters, and important disclaimers can also be mentioned in this section.","title":"Model description"},{"location":"4_3BuildingModelCard/#intended-uses-limitations","text":"Here you describe the use cases the model is intended for, including the languages, fields, and domains where it can be applied. This section of the model card can also document areas that are known to be out of scope for the model, or where it is likely to perform suboptimally.","title":"Intended uses &amp; limitations"},{"location":"4_3BuildingModelCard/#how-to-use","text":"This section should include some examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes, and any other code you think might be helpful.","title":"How to use"},{"location":"4_3BuildingModelCard/#training-data","text":"This part should indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome.","title":"Training data"},{"location":"4_3BuildingModelCard/#training-procedure","text":"In this section you should describe all the relevant aspects of training that are useful from a reproducibility perspective. This includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, the batch size, the learning rate, and so on.","title":"Training procedure"},{"location":"4_3BuildingModelCard/#variable-and-metrics","text":"Here you should describe the metrics you use for evaluation, and the different factors you are mesuring. Mentioning which metric(s) were used, on which dataset and which dataset split, makes it easy to compare you model\u2019s performance compared to that of other models. These should be informed by the previous sections, such as the intended users and use cases.","title":"Variable and metrics"},{"location":"4_3BuildingModelCard/#evaluation-results","text":"Finally, provide an indication of how well the model performs on the evaluation dataset. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses. Model cards are not a requirement when publishing models, and you don\u2019t need to include all of the sections described above when you make one. However, explicit documentation of the model can only benefit future users","title":"Evaluation results"},{"location":"4_3BuildingModelCard/#model-card-metadata","text":"--- language: fr license: mit datasets: - oscar --- This metadata is parsed by the Hugging Face Hub, which then identifies this model as being a French model, with an MIT license, trained on the Oscar dataset.","title":"Model card metadata"},{"location":"5HuggingFaceDatasetsLibrary/","text":"HuggingFace Datasets Library In section 3 you got your first taste of the HuggingFace Datasets library and saw that there were three main steps when it came to fine-tuning a model: Load a dataset from the Hugging Face Hub. Preprocess the data with Dataset.map(). Load and compute metrics. But this is just scratching the surface of what HuggingFace Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\u2019ll find answers to the following questions: What do you do when your dataset is not on the Hub? How can you slice and dice a dataset? (And what if you really need to use Pandas?) What do you do when your dataset is huge and will melt your laptop\u2019s RAM? What the heck are \u201cmemory mapping\u201d and Apache Arrow? How can you create your own dataset and push it to the Hub? Learned to: Load datasets from anywhere, be it the Hugging Face Hub, your laptop, or a remote server at your company. Wrangle your data using a mix of the Dataset.map() and Dataset.filter() functions. Quickly switch between data formats like Pandas and NumPy using Dataset.set_format(). Create your very own dataset and push it to the Hugging Face Hub. Embed your documents using a Transformer model and build a semantic search engine using FAISS.","title":"HuggingFace Datasets Library"},{"location":"5HuggingFaceDatasetsLibrary/#huggingface-datasets-library","text":"In section 3 you got your first taste of the HuggingFace Datasets library and saw that there were three main steps when it came to fine-tuning a model: Load a dataset from the Hugging Face Hub. Preprocess the data with Dataset.map(). Load and compute metrics. But this is just scratching the surface of what HuggingFace Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\u2019ll find answers to the following questions: What do you do when your dataset is not on the Hub? How can you slice and dice a dataset? (And what if you really need to use Pandas?) What do you do when your dataset is huge and will melt your laptop\u2019s RAM? What the heck are \u201cmemory mapping\u201d and Apache Arrow? How can you create your own dataset and push it to the Hub? Learned to: Load datasets from anywhere, be it the Hugging Face Hub, your laptop, or a remote server at your company. Wrangle your data using a mix of the Dataset.map() and Dataset.filter() functions. Quickly switch between data formats like Pandas and NumPy using Dataset.set_format(). Create your very own dataset and push it to the Hugging Face Hub. Embed your documents using a Transformer model and build a semantic search engine using FAISS.","title":"HuggingFace Datasets Library"},{"location":"5_1LoadCustomDataset/","text":"What if my dataset isn't on the Hub? Loading a custom dataset We will explore how the datasets library can be used to load datasets that are not available on the huggingface hub. HuggingFace Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as: load_dataset(\"csv\", data_files=\"my_file.csv\") load_dataset(\"text\", data_files=\"my_file.txt\") load_dataset(\"json\", data_files=\"my_file.jsonl\") load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\") To load a dataset in one of above formats, you just need to provide the name of the format to the load_dataset() function alongwith the data_files argument that points to 1 or more file paths or urls. Loading from local Here's how we can load a local csv dataset. ! wget https : // archive . ics . uci . edu / ml / machine - learning - databases / wine - quality / winequality - white . csv from datasets import load_dataset local_csv_dataset = load_dataset ( \"csv\" , data_files = \"winequality-white.csv\" , sep = \";\" ) local_csv_dataset [ \"train\" ] Here dataset is loaded automatically as a DatasetDict object with each column in the csv file represented as a feature. Loading from remote Remote datasets be loaded by passing URLs to the data_files argument Loading csv files # Load the dataset from the URL directly dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\" remote_csv_dataset = load_dataset ( \"csv\" , data_files = dataset_url , sep = \";\" ) #sep like we pass in pandas dataframe remote_csv_dataset Here, the data_files argument points to a url inside of a local file path Loading raw text files Raw text files are read line by line to build the dataset dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" text_dataset = load_dataset ( \"text\" , data_files = dataset_url ) text_dataset [ \"train\" ][: 5 ] Loading json files Json files can loaded in 2 main ways- either line by line: called json lines where every row in the file is a separate json object for these files you can load the dataset by selecting the json loading script and pointing the data_files argument to the file/url dataset_url = \"https://raw.githubusercontent.com/hirupert/sede/main/data/sede/train.jsonl\" json_lines_dataset = load_dataset ( \"json\" , data_files = dataset_url ) json_lines_dataset [ \"train\" ][: 2 ] - the other json format is by specifying a field in nested JSON - these files basically look like one huge dictionary, so the load_dataset() allows you to specify which specific key to load dataset_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\" json_dataset = load_dataset ( \"json\" , data_files = dataset_url , field = \"data\" ) json_dataset You can also specify which splits to return with the data_files argument If you have more than one split, you can load them by treating data files as a dictionary that maps each split name to its corresponding file. Everythingelse stays completely unchanged url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\" data_files = { \"train\" : f \" { url } train-v2.0.json\" , \"validation\" : f \" { url } dev-v2.0.json\" } json_dataset = load_dataset ( \"json\" , data_files = data_files , field = \"data\" ) json_dataset","title":"Loading Custom Datasets"},{"location":"5_1LoadCustomDataset/#what-if-my-dataset-isnt-on-the-hub","text":"","title":"What if my dataset isn't on the Hub?"},{"location":"5_1LoadCustomDataset/#loading-a-custom-dataset","text":"We will explore how the datasets library can be used to load datasets that are not available on the huggingface hub. HuggingFace Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as: load_dataset(\"csv\", data_files=\"my_file.csv\") load_dataset(\"text\", data_files=\"my_file.txt\") load_dataset(\"json\", data_files=\"my_file.jsonl\") load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\") To load a dataset in one of above formats, you just need to provide the name of the format to the load_dataset() function alongwith the data_files argument that points to 1 or more file paths or urls.","title":"Loading a custom dataset"},{"location":"5_1LoadCustomDataset/#loading-from-local","text":"Here's how we can load a local csv dataset. ! wget https : // archive . ics . uci . edu / ml / machine - learning - databases / wine - quality / winequality - white . csv from datasets import load_dataset local_csv_dataset = load_dataset ( \"csv\" , data_files = \"winequality-white.csv\" , sep = \";\" ) local_csv_dataset [ \"train\" ] Here dataset is loaded automatically as a DatasetDict object with each column in the csv file represented as a feature.","title":"Loading from local"},{"location":"5_1LoadCustomDataset/#loading-from-remote","text":"Remote datasets be loaded by passing URLs to the data_files argument","title":"Loading from remote"},{"location":"5_1LoadCustomDataset/#loading-csv-files","text":"# Load the dataset from the URL directly dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\" remote_csv_dataset = load_dataset ( \"csv\" , data_files = dataset_url , sep = \";\" ) #sep like we pass in pandas dataframe remote_csv_dataset Here, the data_files argument points to a url inside of a local file path","title":"Loading csv files"},{"location":"5_1LoadCustomDataset/#loading-raw-text-files","text":"Raw text files are read line by line to build the dataset dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" text_dataset = load_dataset ( \"text\" , data_files = dataset_url ) text_dataset [ \"train\" ][: 5 ]","title":"Loading raw text files"},{"location":"5_1LoadCustomDataset/#loading-json-files","text":"Json files can loaded in 2 main ways- either line by line: called json lines where every row in the file is a separate json object for these files you can load the dataset by selecting the json loading script and pointing the data_files argument to the file/url dataset_url = \"https://raw.githubusercontent.com/hirupert/sede/main/data/sede/train.jsonl\" json_lines_dataset = load_dataset ( \"json\" , data_files = dataset_url ) json_lines_dataset [ \"train\" ][: 2 ] - the other json format is by specifying a field in nested JSON - these files basically look like one huge dictionary, so the load_dataset() allows you to specify which specific key to load dataset_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\" json_dataset = load_dataset ( \"json\" , data_files = dataset_url , field = \"data\" ) json_dataset You can also specify which splits to return with the data_files argument If you have more than one split, you can load them by treating data files as a dictionary that maps each split name to its corresponding file. Everythingelse stays completely unchanged url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\" data_files = { \"train\" : f \" { url } train-v2.0.json\" , \"validation\" : f \" { url } dev-v2.0.json\" } json_dataset = load_dataset ( \"json\" , data_files = data_files , field = \"data\" ) json_dataset","title":"Loading json files"},{"location":"5_2SliceDiceDataset/","text":"Time to slice and dice Slice and dice a dataset Most of the time the data you work with, won't be perfectly prepared for training models. We will explore the various features that the datasets library provides to clean up your data. The Datasets library provides several methods to filter and transform a dataset. Shuffle and split Select and filter Rename, remove and flatten Map Shuffle and split You can easily shuffle the whole dataset with Dataset.shuffle() from datasets import load_dataset squad = load_dataset ( \"squad\" , split = \"train\" ) squad [ 0 ] squad_shuffled = squad . shuffle ( seed = 666 ) squad_shuffled [ 0 ] It is genrally a good idea to place shuffling to your training set so that your model doesn't learn any artificial ordering the data. Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train_test_split() and specify how large the test split should be. dataset = squad . train_test_split ( test_size = 0.1 ) dataset Select and filter You can return rows according to a list of indices using Dataset.select(). This method expects a list or a generator of the dataset's indices and will then return a new dataset object containing just those rows. indices = [ 0 , 10 , 20 , 40 , 80 ] examples = squad . select ( indices ) examples If you want to create a random sample of rows, you can do this by chaining the shuffle and select methods together. sample = squad . shuffle () . select ( range ( 5 )) sample The last way to pick out specific rows in a dataset is by applying the filter(). This method checks whether each row fulfills some condition or not. squad_filtered = squad . filter ( lambda x : x [ \"title\" ] . startswith ( \"L\" )) squad_filtered [ 0 ] Rename, remove and flatten Use the rename_column() and remove_column() methods to transform your columns. rename_column() to change the name of the column. squad . rename_column ( \"context\" , \"passages\" ) squad remove_column() to delete them squad . remove_columns ([ \"id\" , \"title\" ]) squad Some datasets have nested columns and you can expand these by applying the flatten() squad . flatten () squad Map method the Dataset.map() method applies a custom processing function to each row in the dataset def lowercase_title ( example ): return { \"title\" : example [ \"title\" ] . lower ()} squad_lowercase = squad . map ( lowercase_title ) # Peek at random sample squad_lowercase . shuffle ( seed = 42 )[ \"title\" ][: 5 ] The map() method can also be used to feed batches of rows to the processing function. This is especially useful for tokenization where the tokenizers are backed by the tokenizer library and they can use fast multithreading to process batches in parallel. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased\" ) def tokenize_title ( example ): return tokenizer ( example [ \"title\" ]) squad . map ( tokenize_title , batched = True , batch_size = 500 ) From Datasets to DataFrames and back Although the processing functions of the datasets library will cover most of the cases needed to train a model, there are times when you will need to switch to a library like pandas to access more powerful features or high-level apis for visualizations. Fortunately the datasets library is designed to be inter-operable with libraries like pandas, numpy, pytorch, tensorflow and jax. By default, a Dataset object will return Python objects when you index it. from datasets import load_dataset dataset = load_dataset ( \"swiss_judgment_prediction\" , \"all_languages\" , split = \"train\" ) dataset [ 0 ] but what can you do if you want to ask complex questions about the data? Suppose that before we train any models we would like to explore the data a bit. Explore questions like which legal area is the most common? or How are the languages distributed across regions? Luckily we can use Pandas to answer these questions! Since answering these questions with the native arrow format is not easy. The way this works is that by using the set_format(), we will change the output format of the dataset from python dictionaries to pandas dataframes. # Convert the output format to pandas.DataFrame dataset . set_format ( \"pandas\" ) dataset [ 0 ] the way this words under the hood is that the datasets library changes the magic method getitem () of the dataset. the getitem () method is a special method for python containers that allows you to specify how indexing works. In this case, the getitem () method of the raw dataset starts off by returning a python dictionary and then after applying the set_format() method we change getitem () method to return dataframes instead. dataset . __getitem__ ( 0 ) dataset . set_format ( \"pandas\" ) dataset . __getitem__ ( 0 ) Another way to create a Dataframe is with the Dataset.to_pandas() method. The datasets library also provides a to_pandas() method if you want to do the format conversion and slicing of the datset in one go. df = dataset . to_pandas () df . head () Once we have a DataFrame, we can query our data, make pretty plots and so on. # How are languages distributed across regions? df . groupby ( \"region\" )[ \"language\" ] . value_counts () # Which legal area is most common? df [ \"legal area\" ] . value_counts () Just remember to reset the format back to arrow tables when you are finished. If you don't you can run into the problems if you try to tokenize your text, because it is no longer represented as strings in a dictionary from transformers import AutoTokenizer # Load a pretrained tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) # Tokenize the `text` column dataset . map ( lambda x : tokenizer ( x [ \"text\" ])) By resetting the output format, we get back arrow tables and we can tokenize without problems. # Reset back to Arrow format dataset . reset_format () # Now we can tokenize! dataset . map ( lambda x : tokenizer ( x [ \"text\" ])) Saving and reloading a dataset When you download or process or process a dataset, the processing scripts and data are stored locally in a cache directory. The cache allows the datasets library to avoid re-downloading or processing the entire dataset everytime you use it. Now the data is stored in the form of arrow tables whose location can be found by accessing the dataset's cache_files attribute. from datasets import load_dataset raw_datasets = load_dataset ( \"allocine\" ) raw_datasets . cache_files Alternatively, you can save your dataset in a different location and format. The Datasets library provides 4 main functions to achieve this: Arrow: Dataset.save_to_disk() CSV: Dataset.to_csv() JSON: Dataset.to_json() Parquet: Dataset.to_parquet() CSV and JSON formats are great if you just want to quickly save a small or medium sized dataset. But if your dataset is huge, you will want to save it in either the arrow or parquet formats. Arrow files are great if you plan to reload or process the data in the near future. While parquet files are designed for long term storage and are very space efficient. Arrow format When you save in Arrow format, each split and its metadata are stored in a separate directory. Here we provide the path we wish to save the data to and the datasets library will automatically create a directory for each split to store the arrow table and the metadata. Since we are dealing with a dataset dict object that has multiple splits, this information is also stored in the dataset_dict.json file. raw_datasets . save_to_disk ( \"my-arrow-datasets\" ) my - arrow - datasets / \u251c\u2500\u2500 dataset_dict . json \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 dataset . arrow \u2502 \u251c\u2500\u2500 dataset_info . json \u2502 \u2514\u2500\u2500 state . json \u251c\u2500\u2500 train \u2502 \u251c\u2500\u2500 dataset . arrow \u2502 \u251c\u2500\u2500 dataset_info . json \u2502 \u251c\u2500\u2500 indices . arrow \u2502 \u2514\u2500\u2500 state . json \u2514\u2500\u2500 validation \u251c\u2500\u2500 dataset . arrow \u251c\u2500\u2500 dataset_info . json \u251c\u2500\u2500 indices . arrow \u2514\u2500\u2500 state . json To reload a local Arrow dataset, we use the load_from_disk() function. We simply pass the path of our dataset directory and the original dataset will be recovered. from datasets import load_from_disk arrow_datasets_reloaded = load_from_disk ( \"my-arrow-datasets\" ) arrow_datasets_reloaded CSV Format To save a dataset in CSV format, we save each split as a separate file. We use to_csv() function. In this case, you will need to loop over the splits of the dataset dict object and save each dataset as an individual csv file. Since the o_csv() function is based on the one from pandas, you can pass keyword arguments to configure the output for split , dataset in raw_datasets . items (): dataset . to_csv ( f \"my-dataset- { split } .csv\" , index = None ) We can then reload the CSV files using the data_files argument, together with the csv loading script and the data files argument which specifies the file names associated with each split data_files = { \"train\" : \"my-dataset-train.csv\" , \"validation\" : \"my-dataset-validation.csv\" , \"test\" : \"my-dataset-test.csv\" , } csv_datasets_reloaded = load_dataset ( \"csv\" , data_files = data_files ) csv_datasets_reloaded JSON & Parquet Formats Saving a dataset in JSON and Parquet formats is similar to CSV # Save in JSON Lines format for split , dataset in raw_datasets . items (): dataset . to_json ( f \"my-dataset- { split } .jsonl\" ) # Save in Parquet format for split , dataset in raw_datasets . items (): dataset . to_parquet ( f \"my-dataset- { split } .parquet\" ) We also reload JSON and Parquet files using the data_files argument. json_data_files = { \"train\" : \"my-dataset-train.jsonl\" , \"validation\" : \"my-dataset-validation.jsonl\" , \"test\" : \"my-dataset-test.jsonl\" , } parquet_data_files = { \"train\" : \"my-dataset-train.parquet\" , \"validation\" : \"my-dataset-validation.parquet\" , \"test\" : \"my-dataset-test.parquet\" , } # Reload with the `json` script json_datasets_reloaded = load_dataset ( \"json\" , data_files = json_data_files ) # Reload with the `parquet` script parquet_datasets_reloaded = load_dataset ( \"parquet\" , data_files = parquet_data_files )","title":"Slice And Dice a Dataset"},{"location":"5_2SliceDiceDataset/#time-to-slice-and-dice","text":"","title":"Time to slice and dice"},{"location":"5_2SliceDiceDataset/#slice-and-dice-a-dataset","text":"Most of the time the data you work with, won't be perfectly prepared for training models. We will explore the various features that the datasets library provides to clean up your data. The Datasets library provides several methods to filter and transform a dataset. Shuffle and split Select and filter Rename, remove and flatten Map","title":"Slice and dice a dataset"},{"location":"5_2SliceDiceDataset/#shuffle-and-split","text":"You can easily shuffle the whole dataset with Dataset.shuffle() from datasets import load_dataset squad = load_dataset ( \"squad\" , split = \"train\" ) squad [ 0 ] squad_shuffled = squad . shuffle ( seed = 666 ) squad_shuffled [ 0 ] It is genrally a good idea to place shuffling to your training set so that your model doesn't learn any artificial ordering the data. Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train_test_split() and specify how large the test split should be. dataset = squad . train_test_split ( test_size = 0.1 ) dataset","title":"Shuffle and split"},{"location":"5_2SliceDiceDataset/#select-and-filter","text":"You can return rows according to a list of indices using Dataset.select(). This method expects a list or a generator of the dataset's indices and will then return a new dataset object containing just those rows. indices = [ 0 , 10 , 20 , 40 , 80 ] examples = squad . select ( indices ) examples If you want to create a random sample of rows, you can do this by chaining the shuffle and select methods together. sample = squad . shuffle () . select ( range ( 5 )) sample The last way to pick out specific rows in a dataset is by applying the filter(). This method checks whether each row fulfills some condition or not. squad_filtered = squad . filter ( lambda x : x [ \"title\" ] . startswith ( \"L\" )) squad_filtered [ 0 ]","title":"Select and filter"},{"location":"5_2SliceDiceDataset/#rename-remove-and-flatten","text":"Use the rename_column() and remove_column() methods to transform your columns. rename_column() to change the name of the column. squad . rename_column ( \"context\" , \"passages\" ) squad remove_column() to delete them squad . remove_columns ([ \"id\" , \"title\" ]) squad Some datasets have nested columns and you can expand these by applying the flatten() squad . flatten () squad","title":"Rename, remove and flatten"},{"location":"5_2SliceDiceDataset/#map-method","text":"the Dataset.map() method applies a custom processing function to each row in the dataset def lowercase_title ( example ): return { \"title\" : example [ \"title\" ] . lower ()} squad_lowercase = squad . map ( lowercase_title ) # Peek at random sample squad_lowercase . shuffle ( seed = 42 )[ \"title\" ][: 5 ] The map() method can also be used to feed batches of rows to the processing function. This is especially useful for tokenization where the tokenizers are backed by the tokenizer library and they can use fast multithreading to process batches in parallel. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased\" ) def tokenize_title ( example ): return tokenizer ( example [ \"title\" ]) squad . map ( tokenize_title , batched = True , batch_size = 500 )","title":"Map method"},{"location":"5_2SliceDiceDataset/#from-datasets-to-dataframes-and-back","text":"Although the processing functions of the datasets library will cover most of the cases needed to train a model, there are times when you will need to switch to a library like pandas to access more powerful features or high-level apis for visualizations. Fortunately the datasets library is designed to be inter-operable with libraries like pandas, numpy, pytorch, tensorflow and jax. By default, a Dataset object will return Python objects when you index it. from datasets import load_dataset dataset = load_dataset ( \"swiss_judgment_prediction\" , \"all_languages\" , split = \"train\" ) dataset [ 0 ] but what can you do if you want to ask complex questions about the data? Suppose that before we train any models we would like to explore the data a bit. Explore questions like which legal area is the most common? or How are the languages distributed across regions? Luckily we can use Pandas to answer these questions! Since answering these questions with the native arrow format is not easy. The way this works is that by using the set_format(), we will change the output format of the dataset from python dictionaries to pandas dataframes. # Convert the output format to pandas.DataFrame dataset . set_format ( \"pandas\" ) dataset [ 0 ] the way this words under the hood is that the datasets library changes the magic method getitem () of the dataset. the getitem () method is a special method for python containers that allows you to specify how indexing works. In this case, the getitem () method of the raw dataset starts off by returning a python dictionary and then after applying the set_format() method we change getitem () method to return dataframes instead. dataset . __getitem__ ( 0 ) dataset . set_format ( \"pandas\" ) dataset . __getitem__ ( 0 ) Another way to create a Dataframe is with the Dataset.to_pandas() method. The datasets library also provides a to_pandas() method if you want to do the format conversion and slicing of the datset in one go. df = dataset . to_pandas () df . head () Once we have a DataFrame, we can query our data, make pretty plots and so on. # How are languages distributed across regions? df . groupby ( \"region\" )[ \"language\" ] . value_counts () # Which legal area is most common? df [ \"legal area\" ] . value_counts () Just remember to reset the format back to arrow tables when you are finished. If you don't you can run into the problems if you try to tokenize your text, because it is no longer represented as strings in a dictionary from transformers import AutoTokenizer # Load a pretrained tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) # Tokenize the `text` column dataset . map ( lambda x : tokenizer ( x [ \"text\" ])) By resetting the output format, we get back arrow tables and we can tokenize without problems. # Reset back to Arrow format dataset . reset_format () # Now we can tokenize! dataset . map ( lambda x : tokenizer ( x [ \"text\" ]))","title":"From Datasets to DataFrames and back"},{"location":"5_2SliceDiceDataset/#saving-and-reloading-a-dataset","text":"When you download or process or process a dataset, the processing scripts and data are stored locally in a cache directory. The cache allows the datasets library to avoid re-downloading or processing the entire dataset everytime you use it. Now the data is stored in the form of arrow tables whose location can be found by accessing the dataset's cache_files attribute. from datasets import load_dataset raw_datasets = load_dataset ( \"allocine\" ) raw_datasets . cache_files Alternatively, you can save your dataset in a different location and format. The Datasets library provides 4 main functions to achieve this: Arrow: Dataset.save_to_disk() CSV: Dataset.to_csv() JSON: Dataset.to_json() Parquet: Dataset.to_parquet() CSV and JSON formats are great if you just want to quickly save a small or medium sized dataset. But if your dataset is huge, you will want to save it in either the arrow or parquet formats. Arrow files are great if you plan to reload or process the data in the near future. While parquet files are designed for long term storage and are very space efficient.","title":"Saving and reloading a dataset"},{"location":"5_2SliceDiceDataset/#arrow-format","text":"When you save in Arrow format, each split and its metadata are stored in a separate directory. Here we provide the path we wish to save the data to and the datasets library will automatically create a directory for each split to store the arrow table and the metadata. Since we are dealing with a dataset dict object that has multiple splits, this information is also stored in the dataset_dict.json file. raw_datasets . save_to_disk ( \"my-arrow-datasets\" ) my - arrow - datasets / \u251c\u2500\u2500 dataset_dict . json \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 dataset . arrow \u2502 \u251c\u2500\u2500 dataset_info . json \u2502 \u2514\u2500\u2500 state . json \u251c\u2500\u2500 train \u2502 \u251c\u2500\u2500 dataset . arrow \u2502 \u251c\u2500\u2500 dataset_info . json \u2502 \u251c\u2500\u2500 indices . arrow \u2502 \u2514\u2500\u2500 state . json \u2514\u2500\u2500 validation \u251c\u2500\u2500 dataset . arrow \u251c\u2500\u2500 dataset_info . json \u251c\u2500\u2500 indices . arrow \u2514\u2500\u2500 state . json To reload a local Arrow dataset, we use the load_from_disk() function. We simply pass the path of our dataset directory and the original dataset will be recovered. from datasets import load_from_disk arrow_datasets_reloaded = load_from_disk ( \"my-arrow-datasets\" ) arrow_datasets_reloaded","title":"Arrow format"},{"location":"5_2SliceDiceDataset/#csv-format","text":"To save a dataset in CSV format, we save each split as a separate file. We use to_csv() function. In this case, you will need to loop over the splits of the dataset dict object and save each dataset as an individual csv file. Since the o_csv() function is based on the one from pandas, you can pass keyword arguments to configure the output for split , dataset in raw_datasets . items (): dataset . to_csv ( f \"my-dataset- { split } .csv\" , index = None ) We can then reload the CSV files using the data_files argument, together with the csv loading script and the data files argument which specifies the file names associated with each split data_files = { \"train\" : \"my-dataset-train.csv\" , \"validation\" : \"my-dataset-validation.csv\" , \"test\" : \"my-dataset-test.csv\" , } csv_datasets_reloaded = load_dataset ( \"csv\" , data_files = data_files ) csv_datasets_reloaded","title":"CSV Format"},{"location":"5_2SliceDiceDataset/#json-parquet-formats","text":"Saving a dataset in JSON and Parquet formats is similar to CSV # Save in JSON Lines format for split , dataset in raw_datasets . items (): dataset . to_json ( f \"my-dataset- { split } .jsonl\" ) # Save in Parquet format for split , dataset in raw_datasets . items (): dataset . to_parquet ( f \"my-dataset- { split } .parquet\" ) We also reload JSON and Parquet files using the data_files argument. json_data_files = { \"train\" : \"my-dataset-train.jsonl\" , \"validation\" : \"my-dataset-validation.jsonl\" , \"test\" : \"my-dataset-test.jsonl\" , } parquet_data_files = { \"train\" : \"my-dataset-train.parquet\" , \"validation\" : \"my-dataset-validation.parquet\" , \"test\" : \"my-dataset-test.parquet\" , } # Reload with the `json` script json_datasets_reloaded = load_dataset ( \"json\" , data_files = json_data_files ) # Reload with the `parquet` script parquet_datasets_reloaded = load_dataset ( \"parquet\" , data_files = parquet_data_files )","title":"JSON &amp; Parquet Formats"},{"location":"5_3BigData/","text":"Big data? HuggingFace Datasets to the rescue! We will take a look at 2 core features of the Datasets library that allow to load and process huge datasets without blowing up laptop, cpu. If you want to train a model from scratch, you will need a LOT of data. Nowadays it is not uncommon to find yourself with multi GB-sized datasets, especially if you are planning to pre-train a transformer like Bert or GPT2 from scratch. In these cases, even loading the data can be a challenge. For example, the C4 corpus used to pretrained T5 consists of over 2 TB of data. To handle these large datasets, the Datasets library is built on 2 core features: apache Arrow format and streaming api. Datasets library uses Arrow and streaming to handle data at scale. Arrow is designed for high performance data processing and represents each table-like dataset with a column-based format. eg here: column-based formats group the elements of a table in consecutive blocks on RAM and this unlocks fast access and processing. Arrow Arrow is great at processing data at any scale but some datasets are so large that you can not even fit them on your hard disk. So for these cases, the Datasets library provides a streaming API that allows you to progressively download the raw data one element at a time. The result is a special object called an iterable dataset. Arrow's memory-mapped format enables access to bigger-than-RAM datasets. Arrow is powerful. Its first feature is that it treats every dataset as a memory-map file. Now memory-mapping is a mechanism that maps a portion of a file or an entire file and disk to a chunk of virtual memory. This allows applications to access segments of an extremely large file without having to read the whole file into memory first. from datasets import load_dataset data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\" large_dataset = load_dataset ( \"json\" , data_files = data_files , split = \"train\" ) size_gb = large_dataset . dataset_size / ( 1024 ** 3 ) print ( f \"Dataset size (cache file) : { size_gb : .2f } GB\" ) import psutil # Process.memory_info is expressed in bytes, so convert to megabytes print ( f \"RAM used: { psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) : .2f } MB\" ) Memory-mapped files can be shared across multiple processes. Another cool feature of Arrow is memory-mapping capabilities is that it allows multiple processors to work with the same large dataset without moving it or copying it in any way. This zero copy feature of Arrow makes it extremely fast for iterating over a dataset. Below we iterate over 15 Million rows in about a minute just using a standard laptop. import timeit code_snippet = \"\"\"batch_size = 1000 for idx in range(0, len(large_dataset), batch_size): _ = large_dataset[idx:idx + batch_size] \"\"\" time = timeit . timeit ( stmt = code_snippet , number = 1 , globals = globals ()) print ( f \"Iterated over { len ( large_dataset ) } examples (about { size_gb : .1f } GB) in \" f \" { time : .1f } s, i.e. { size_gb / time : .3f } GB/s\" ) Streaming Streaming lets you process bigger-than-disk datasets. How to stream a large dataset - the only change need to make is to set the streaming=True argument in the load_dataset() function. This will return a special iterable dataset object which is a bit different to the dataset objects we have seen till now. This object is an iterable which means we can not index it to access elements but instead we iterate on it using iter() and next() methods. This will download and access a single example from the dataset which means we can progressively iterate through a huge dataset without having to download it first. large_dataset_streamed = load_dataset ( \"json\" , data_files = data_files , split = \"train\" , streaming = True ) next ( iter ( large_dataset_streamed )) type ( large_dataset_streamed ) Tokenization with IterableDataset.map() works the same way too. Tokenizing text with the map() method also works in a similar way. We first stream the dataset and then apply the map() method with the tokenizer. To get the first tokenized example, we apply iter() and next() methods. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased\" ) tokenized_dataset = large_dataset_streamed . map ( lambda x : tokenizer ( x [ \"text\" ])) next ( iter ( tokenized_dataset )) ... but instead of select() we use take() and skip() The main difference with an iterable dataset is that instead of using a select() to return examples, we use take() and skip() because we can not index into the dataset. The take() method returns the first n examples in the dataset while skip() method skips the first n and retirns the rest. # Select the first 5 examples dataset_head = large_dataset_streamed . take ( 5 ) list ( dataset_head ) # Skip the first 1,000 examples and include the rest in the training set train_dataset = large_dataset_streamed . skip ( 1000 ) # Take the first 1,000 examples for the validation set validation_dataset = large_dataset_streamed . take ( 1000 )","title":"Big Data with Datasets"},{"location":"5_3BigData/#big-data-huggingface-datasets-to-the-rescue","text":"We will take a look at 2 core features of the Datasets library that allow to load and process huge datasets without blowing up laptop, cpu. If you want to train a model from scratch, you will need a LOT of data. Nowadays it is not uncommon to find yourself with multi GB-sized datasets, especially if you are planning to pre-train a transformer like Bert or GPT2 from scratch. In these cases, even loading the data can be a challenge. For example, the C4 corpus used to pretrained T5 consists of over 2 TB of data. To handle these large datasets, the Datasets library is built on 2 core features: apache Arrow format and streaming api. Datasets library uses Arrow and streaming to handle data at scale. Arrow is designed for high performance data processing and represents each table-like dataset with a column-based format. eg here: column-based formats group the elements of a table in consecutive blocks on RAM and this unlocks fast access and processing.","title":"Big data? HuggingFace Datasets to the rescue!"},{"location":"5_3BigData/#arrow","text":"Arrow is great at processing data at any scale but some datasets are so large that you can not even fit them on your hard disk. So for these cases, the Datasets library provides a streaming API that allows you to progressively download the raw data one element at a time. The result is a special object called an iterable dataset. Arrow's memory-mapped format enables access to bigger-than-RAM datasets. Arrow is powerful. Its first feature is that it treats every dataset as a memory-map file. Now memory-mapping is a mechanism that maps a portion of a file or an entire file and disk to a chunk of virtual memory. This allows applications to access segments of an extremely large file without having to read the whole file into memory first. from datasets import load_dataset data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\" large_dataset = load_dataset ( \"json\" , data_files = data_files , split = \"train\" ) size_gb = large_dataset . dataset_size / ( 1024 ** 3 ) print ( f \"Dataset size (cache file) : { size_gb : .2f } GB\" ) import psutil # Process.memory_info is expressed in bytes, so convert to megabytes print ( f \"RAM used: { psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) : .2f } MB\" ) Memory-mapped files can be shared across multiple processes. Another cool feature of Arrow is memory-mapping capabilities is that it allows multiple processors to work with the same large dataset without moving it or copying it in any way. This zero copy feature of Arrow makes it extremely fast for iterating over a dataset. Below we iterate over 15 Million rows in about a minute just using a standard laptop. import timeit code_snippet = \"\"\"batch_size = 1000 for idx in range(0, len(large_dataset), batch_size): _ = large_dataset[idx:idx + batch_size] \"\"\" time = timeit . timeit ( stmt = code_snippet , number = 1 , globals = globals ()) print ( f \"Iterated over { len ( large_dataset ) } examples (about { size_gb : .1f } GB) in \" f \" { time : .1f } s, i.e. { size_gb / time : .3f } GB/s\" )","title":"Arrow"},{"location":"5_3BigData/#streaming","text":"Streaming lets you process bigger-than-disk datasets. How to stream a large dataset - the only change need to make is to set the streaming=True argument in the load_dataset() function. This will return a special iterable dataset object which is a bit different to the dataset objects we have seen till now. This object is an iterable which means we can not index it to access elements but instead we iterate on it using iter() and next() methods. This will download and access a single example from the dataset which means we can progressively iterate through a huge dataset without having to download it first. large_dataset_streamed = load_dataset ( \"json\" , data_files = data_files , split = \"train\" , streaming = True ) next ( iter ( large_dataset_streamed )) type ( large_dataset_streamed ) Tokenization with IterableDataset.map() works the same way too. Tokenizing text with the map() method also works in a similar way. We first stream the dataset and then apply the map() method with the tokenizer. To get the first tokenized example, we apply iter() and next() methods. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased\" ) tokenized_dataset = large_dataset_streamed . map ( lambda x : tokenizer ( x [ \"text\" ])) next ( iter ( tokenized_dataset )) ... but instead of select() we use take() and skip() The main difference with an iterable dataset is that instead of using a select() to return examples, we use take() and skip() because we can not index into the dataset. The take() method returns the first n examples in the dataset while skip() method skips the first n and retirns the rest. # Select the first 5 examples dataset_head = large_dataset_streamed . take ( 5 ) list ( dataset_head ) # Skip the first 1,000 examples and include the rest in the training set train_dataset = large_dataset_streamed . skip ( 1000 ) # Take the first 1,000 examples for the validation set validation_dataset = large_dataset_streamed . take ( 1000 )","title":"Streaming"},{"location":"5_4CreatingOwnDataset/","text":"Creating your own dataset Uploading the dataset to the Hugging Face Hub Step 1: Create a repository to host all your files The first thing you need to do is create a new dataset repository on the hub. So just click on your profile icon and select the New Dataset option. Next we need to assign an owner of the dataset. By default, this will be your hub account but you can also create datasets under any organization that you belong to Then we just need to give the dataset a good name specify whether it is a public or private dataset Public datasets can be accessed by anyone while private datasets can only be accessed by you or members of your organization With that we can go ahead and create the dataset. Step 2: Upload your files Now that you have an empty dataset repository on the hub, the next thing to do is add some actual data to it. You can do this with git but the easiest way is by selecting the upload file button Then you can just go ahead and upload the files directly from your machine. After you have uploaded your files, you will see them appear in the repository under the files and the versions tab. Step 3: Create a dataset card The last step is to create a dataset card Well documented datasets are more likely to be usefuls to others as they provide the context to decide whether the dataset is relevant or whether there are any biases or risks associated with using the dataset. On the huggingface hub this information is stored in each repository's Readme file. There are 2 main steps that you should take: First you neeed to create some metadata that will allow your dataset to be easily found by others on the hub You can create this metadata using the dataset's tag and application Once you have created the metadata you can fill out the rest of the dataset card Step 4: Load your dataset and have fun! Once your dataset is on the hub you can load it using the load_dataset() function Just provide the name of your repostory and a data_files and you are good to go from datasets import load_dataset data_files = { \"train\" : \"train.csv\" , \"test\" : \"test.csv\" } my_dataset = load_dataset ( \"pallavi176/my-awesome-dataset\" , data_files = data_files ) my_dataset","title":"Creating Your Own Dataset"},{"location":"5_4CreatingOwnDataset/#creating-your-own-dataset","text":"","title":"Creating your own dataset"},{"location":"5_4CreatingOwnDataset/#uploading-the-dataset-to-the-hugging-face-hub","text":"","title":"Uploading the dataset to the Hugging Face Hub"},{"location":"5_4CreatingOwnDataset/#step-1-create-a-repository-to-host-all-your-files","text":"The first thing you need to do is create a new dataset repository on the hub. So just click on your profile icon and select the New Dataset option. Next we need to assign an owner of the dataset. By default, this will be your hub account but you can also create datasets under any organization that you belong to Then we just need to give the dataset a good name specify whether it is a public or private dataset Public datasets can be accessed by anyone while private datasets can only be accessed by you or members of your organization With that we can go ahead and create the dataset.","title":"Step 1: Create a repository to host all your files"},{"location":"5_4CreatingOwnDataset/#step-2-upload-your-files","text":"Now that you have an empty dataset repository on the hub, the next thing to do is add some actual data to it. You can do this with git but the easiest way is by selecting the upload file button Then you can just go ahead and upload the files directly from your machine. After you have uploaded your files, you will see them appear in the repository under the files and the versions tab.","title":"Step 2: Upload your files"},{"location":"5_4CreatingOwnDataset/#step-3-create-a-dataset-card","text":"The last step is to create a dataset card Well documented datasets are more likely to be usefuls to others as they provide the context to decide whether the dataset is relevant or whether there are any biases or risks associated with using the dataset. On the huggingface hub this information is stored in each repository's Readme file. There are 2 main steps that you should take: First you neeed to create some metadata that will allow your dataset to be easily found by others on the hub You can create this metadata using the dataset's tag and application Once you have created the metadata you can fill out the rest of the dataset card","title":"Step 3: Create a dataset card"},{"location":"5_4CreatingOwnDataset/#step-4-load-your-dataset-and-have-fun","text":"Once your dataset is on the hub you can load it using the load_dataset() function Just provide the name of your repostory and a data_files and you are good to go from datasets import load_dataset data_files = { \"train\" : \"train.csv\" , \"test\" : \"test.csv\" } my_dataset = load_dataset ( \"pallavi176/my-awesome-dataset\" , data_files = data_files ) my_dataset","title":"Step 4: Load your dataset and have fun!"},{"location":"5_5SemanticSearchWithFAISS/","text":"Semantic search with FAISS Text embeddings & semantic search How transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text embeddings represent text as vectors. Text Embeddings are just a fancy way of saying that we can represent text as an array of numbers called a vector. To create these embeddings, we usually use an encoder-based model like BERT. We can use metrics like cosine similarity to compare how close two embeddings are. The trick to do the comparison is to compute a similarity metric between each pair of embedding vectors. These vectors usually live in a very high dimensional space. So a similarity metrics can be anything that measures some sort of distance between vectors. One very popular metric is cosine similarity which uses the angle between two vectors to measure how close they are. Each token is represented by one vector, but we want one vector per sentence. ie, One problem we have to deal with is that transformer models like bert will actually return one embedding vector per token. In below example, the output of model has produced 9 embedding vectors per sentence and each vector has 384 dimensions. But what we really want is a single embedding vector for each sentence. import torch from transformers import AutoTokenizer , AutoModel sentences = [ \"I took my dog for a walk\" , \"Today is going to rain\" , \"I took my cat for a walk\" , ] model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer . from_pretrained ( model_ckpt ) model = AutoModel . from_pretrained ( model_ckpt ) encoded_input = tokenizer ( sentences , padding = True , truncation = True , return_tensors = \"pt\" ) with torch . no_grad (): model_output = model ( ** encoded_input ) token_embeddings = model_output . last_hidden_state print ( f \"Token embeddings shape: { token_embeddings . size () } \" ) # [3,9,384] [num_sentence, num_tokens, embed_dim] Use mean pooling to create the sentence vectors! To deal with this we can use a technique called pooling. The simplest pulling method is just to take the token embedding of the special [CLS] token. Alternatively we can average the token embeddings which is called mean pooling. With mean pooling the only thing we need to make sure is that we don't include the padding tokens in the average which is why you can see the attention mask being used here. This gives us a 384 dimensional vector for each sentence which is exactly what we want. import torch.nn.functional as F def mean_pooling ( model_output , attention_mask ): token_embeddings = model_output . last_hidden_state input_mask_expanded = ( attention_mask . unsqueeze ( - 1 ) . expand ( token_embeddings . size ()) . float () ) return torch . sum ( token_embeddings * input_mask_expanded , 1 ) / torch . clamp ( input_mask_expanded . sum ( 1 ), min = 1e-9 ) sentence_embeddings = mean_pooling ( model_output , encoded_input [ \"attention_mask\" ]) # Normalize the embeddings sentence_embeddings = F . normalize ( sentence_embeddings , p = 2 , dim = 1 ) print ( f \"Sentence embeddings shape: { sentence_embeddings . size () } \" ) # [3,384] [num_sentences, embed_dim] And once we have our sentence embeddings we can calculate the cosine similarity for each pair of vectors. Below, we used sklearn function to calculate cosine similarity. import numpy as np from sklearn.metrics.pairwise import cosine_similarity sentence_embeddings = sentence_embeddings . detach () . numpy () scores = np . zeros (( sentence_embeddings . shape [ 0 ], sentence_embeddings . shape [ 0 ])) for idx in range ( sentence_embeddings . shape [ 0 ]): scores [ idx , :] = cosine_similarity ([ sentence_embeddings [ idx ]], sentence_embeddings )[ 0 ] You can use the same trick to measure similarity of query against a corpus of docs. We can actually take this idea one step further by comparing the similarity between a question and a corpus of documents For example, suppose we embed every post in the huggingface forums, we can then ask a question, embed it and check which forum posts are similar. This process is often called semantic search because it allows us to compare queries with context. To create a semantic search engine is actually quite simple in the Datasets library. First we need to embed all the documents. Below we take a small sample from the squad dataset and apply the same embedding logic as before. This gives us a new column called embeddings which stores the embeddings of every passage. from datasets import load_dataset squad = load_dataset ( \"squad\" , split = \"validation\" ) . shuffle ( seed = 42 ) . select ( range ( 100 )) def get_embeddings ( text_list ): encoded_input = tokenizer ( text_list , padding = True , truncation = True , return_tensors = \"pt\" ) encoded_input = { k : v for k , v in encoded_input . items ()} with torch . no_grad (): model_output = model ( ** encoded_input ) return mean_pooling ( model_output , encoded_input [ \"attention_mask\" ]) squad_with_embeddings = squad . map ( lambda x : { \"embeddings\" : get_embeddings ( x [ \"context\" ]) . cpu () . numpy ()[ 0 ]} ) We use a special FAISS index for fast nearest neighbour lookup. Once we have our embeddings, we need a way to find nearest neighbors for a query. The Datasets library provides a special object called FAISS which allows you to quickly compare embedding vectors. So we add the FAISS index, embed a question and we found the 3 most similar articles which might store the answer. squad_with_embeddings . add_faiss_index ( column = \"embeddings\" ) question = \"Who headlined the halftime show for Super Bowl 50?\" question_embedding = get_embeddings ([ question ]) . cpu () . detach () . numpy () scores , samples = squad_with_embeddings . get_nearest_examples ( \"embeddings\" , question_embedding , k = 3 )","title":"Semantic search with FAISS"},{"location":"5_5SemanticSearchWithFAISS/#semantic-search-with-faiss","text":"","title":"Semantic search with FAISS"},{"location":"5_5SemanticSearchWithFAISS/#text-embeddings-semantic-search","text":"How transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text embeddings represent text as vectors. Text Embeddings are just a fancy way of saying that we can represent text as an array of numbers called a vector. To create these embeddings, we usually use an encoder-based model like BERT. We can use metrics like cosine similarity to compare how close two embeddings are. The trick to do the comparison is to compute a similarity metric between each pair of embedding vectors. These vectors usually live in a very high dimensional space. So a similarity metrics can be anything that measures some sort of distance between vectors. One very popular metric is cosine similarity which uses the angle between two vectors to measure how close they are. Each token is represented by one vector, but we want one vector per sentence. ie, One problem we have to deal with is that transformer models like bert will actually return one embedding vector per token. In below example, the output of model has produced 9 embedding vectors per sentence and each vector has 384 dimensions. But what we really want is a single embedding vector for each sentence. import torch from transformers import AutoTokenizer , AutoModel sentences = [ \"I took my dog for a walk\" , \"Today is going to rain\" , \"I took my cat for a walk\" , ] model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer . from_pretrained ( model_ckpt ) model = AutoModel . from_pretrained ( model_ckpt ) encoded_input = tokenizer ( sentences , padding = True , truncation = True , return_tensors = \"pt\" ) with torch . no_grad (): model_output = model ( ** encoded_input ) token_embeddings = model_output . last_hidden_state print ( f \"Token embeddings shape: { token_embeddings . size () } \" ) # [3,9,384] [num_sentence, num_tokens, embed_dim] Use mean pooling to create the sentence vectors! To deal with this we can use a technique called pooling. The simplest pulling method is just to take the token embedding of the special [CLS] token. Alternatively we can average the token embeddings which is called mean pooling. With mean pooling the only thing we need to make sure is that we don't include the padding tokens in the average which is why you can see the attention mask being used here. This gives us a 384 dimensional vector for each sentence which is exactly what we want. import torch.nn.functional as F def mean_pooling ( model_output , attention_mask ): token_embeddings = model_output . last_hidden_state input_mask_expanded = ( attention_mask . unsqueeze ( - 1 ) . expand ( token_embeddings . size ()) . float () ) return torch . sum ( token_embeddings * input_mask_expanded , 1 ) / torch . clamp ( input_mask_expanded . sum ( 1 ), min = 1e-9 ) sentence_embeddings = mean_pooling ( model_output , encoded_input [ \"attention_mask\" ]) # Normalize the embeddings sentence_embeddings = F . normalize ( sentence_embeddings , p = 2 , dim = 1 ) print ( f \"Sentence embeddings shape: { sentence_embeddings . size () } \" ) # [3,384] [num_sentences, embed_dim] And once we have our sentence embeddings we can calculate the cosine similarity for each pair of vectors. Below, we used sklearn function to calculate cosine similarity. import numpy as np from sklearn.metrics.pairwise import cosine_similarity sentence_embeddings = sentence_embeddings . detach () . numpy () scores = np . zeros (( sentence_embeddings . shape [ 0 ], sentence_embeddings . shape [ 0 ])) for idx in range ( sentence_embeddings . shape [ 0 ]): scores [ idx , :] = cosine_similarity ([ sentence_embeddings [ idx ]], sentence_embeddings )[ 0 ] You can use the same trick to measure similarity of query against a corpus of docs. We can actually take this idea one step further by comparing the similarity between a question and a corpus of documents For example, suppose we embed every post in the huggingface forums, we can then ask a question, embed it and check which forum posts are similar. This process is often called semantic search because it allows us to compare queries with context. To create a semantic search engine is actually quite simple in the Datasets library. First we need to embed all the documents. Below we take a small sample from the squad dataset and apply the same embedding logic as before. This gives us a new column called embeddings which stores the embeddings of every passage. from datasets import load_dataset squad = load_dataset ( \"squad\" , split = \"validation\" ) . shuffle ( seed = 42 ) . select ( range ( 100 )) def get_embeddings ( text_list ): encoded_input = tokenizer ( text_list , padding = True , truncation = True , return_tensors = \"pt\" ) encoded_input = { k : v for k , v in encoded_input . items ()} with torch . no_grad (): model_output = model ( ** encoded_input ) return mean_pooling ( model_output , encoded_input [ \"attention_mask\" ]) squad_with_embeddings = squad . map ( lambda x : { \"embeddings\" : get_embeddings ( x [ \"context\" ]) . cpu () . numpy ()[ 0 ]} ) We use a special FAISS index for fast nearest neighbour lookup. Once we have our embeddings, we need a way to find nearest neighbors for a query. The Datasets library provides a special object called FAISS which allows you to quickly compare embedding vectors. So we add the FAISS index, embed a question and we found the 3 most similar articles which might store the answer. squad_with_embeddings . add_faiss_index ( column = \"embeddings\" ) question = \"Who headlined the halftime show for Super Bowl 50?\" question_embedding = get_embeddings ([ question ]) . cpu () . detach () . numpy () scores , samples = squad_with_embeddings . get_nearest_examples ( \"embeddings\" , question_embedding , k = 3 )","title":"Text embeddings &amp; semantic search"},{"location":"6HuggingFaceTokenizersLibrary/","text":"HuggingFace Tokenizers Library Introduction We will learn: How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts The special features of fast tokenizers The differences between the three main subword tokenization algorithms used in NLP today How to build a tokenizer from scratch with the HuggingFace Tokenizers library and train it on some data After this deep dive into tokenizers, you should: Be able to train a new tokenizer using an old one as a template Understand how to use offsets to map tokens\u2019 positions to their original span of text Know the differences between BPE, WordPiece, and Unigram Be able to mix and match the blocks provided by the \ud83e\udd17 Tokenizers library to build your own tokenizer Be able to use that tokenizer inside the \ud83e\udd17 Transformers library","title":"HuggingFace Tokenizers Library"},{"location":"6HuggingFaceTokenizersLibrary/#huggingface-tokenizers-library","text":"","title":"HuggingFace Tokenizers Library"},{"location":"6HuggingFaceTokenizersLibrary/#introduction","text":"We will learn: How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts The special features of fast tokenizers The differences between the three main subword tokenization algorithms used in NLP today How to build a tokenizer from scratch with the HuggingFace Tokenizers library and train it on some data After this deep dive into tokenizers, you should: Be able to train a new tokenizer using an old one as a template Understand how to use offsets to map tokens\u2019 positions to their original span of text Know the differences between BPE, WordPiece, and Unigram Be able to mix and match the blocks provided by the \ud83e\udd17 Tokenizers library to build your own tokenizer Be able to use that tokenizer inside the \ud83e\udd17 Transformers library","title":"Introduction"},{"location":"6_1TrainingNewTokenizerFromOld1/","text":"Training a new tokenizer from an old one Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It\u2019s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It\u2019s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus. Training a new tokenizer What is the purpose of training a tokenizer What are the key steps to follow What is the easiest way is to it. Should I train a new tokenizer when you plan to train a new model from scratch. You may want to consider training a new tokenizer so that you have a tokenizer suitable for the training corpus used to train a language model from scratch. A tokenizer will not be suitable if it has been trained on a corpus that is not similar to the one you will use to train your model from scratch. A trained tokenizer will not be suitable for your corpus if your corpus is in a different language, use new characters such as accent, etc, and a specific vocabulary, for instance medical or legal, or use a different style, eg, language of another country. Dissimilarities can arise from: New language New characters New domain New style If I take the tokenizer trained on bert-based-uncased model on ignore and normalized step, then we can say that the tokenization operations in the english sentence, \"here is a sentence adapted to our tokenizer\", produces a set of english tokens. from transformers import BertTokenizerFast tokenizer = BertTokenizerFast . from_pretrained ( 'huggingface-course/bert-base-uncased-tokenizer-without-normalizer' ) Here, a sentence of 8 words produces a tokens list of length 9. text = \"here is a sentence adapted to our tokenizer\" print ( tokenizer . tokenize ( text )) You may want to consider training a new tokenizer so that you have a tokenizer suitable for the training corpus used to train a language model from scratch. If I use the same tokenizer on the sentence in bengali, the words will be divided into may sub-tokens since tokenizes does not know the unicode characters. text = \"\u098f\u0987 \u09ac\u09be\u0995\u09cd\u09af\u099f\u09bf \u0986\u09ae\u09be\u09a6\u09c7\u09b0 \u099f\u09cb\u0995\u09c7\u09a8\u09be\u0987\u099c\u09be\u09b0\u09c7\u09b0 \u0989\u09aa\u09af\u09c1\u0995\u09cd\u09a4 \u09a8\u09af\u09bc\" print ( tokenizer . tokenize ( text )) The fact that the common word is split into many sub-tokens can be problemmatic. Because language models can only handle a sequence of tokens of limited length. The tokens that are excessively split the initial text may even impact the performance of the model. [UNK] tokens are also problemmatic because the model will not be able to extract any information from them. Another example, we see tokenizers replaces words containing characters on \u00e0cc\u00ebnts or CAPITAL LETTERS with [UNK] tokens text = \"this tokenizer does not know \u00e0cc\u00ebnts and CAPITAL LETTERS\" print ( tokenizer . tokenize ( text )) Finally, if we use this tokenizers to tokenize medical vocabulary, we see that a single word is divided into many smaller tokens. text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\" print ( tokenizer . tokenize ( text )) Most of the tokenizers used by the current State-of-the-Art language models need to be trained on a corpus that is allowed to one used to pre-train a language model. This training consists of learning rules to divide text into tokens On the way to learn these rules the token use them depends on the chosen tokenizer model. The procedural for training a tokenizer can be summarized in these main steps: Gathering a corpus of text Choosen a tokenizer architecture Train the tokenizer on the corpus Save the result Let's say you wanted to train a GPT-2 model on python code. example = \"\"\"class LinearLayer(): def __init__(self, input_size, output_size): self.weight = torch.randn(input_size, output_size) self.bias = torch.zeros(output_size) def __call__(self, x): return x @ self.weights + self.bias \"\"\" print ( old_tokenizer . tokenize ( example )) print ( new_tokenizer . tokenize ( example )) The Transformers library provides a very easy to use method to train a tokenizer using a known architecture on a new corpus. AutoTokenizer . train_new_from_iterator ( text_iterator , vocab_size , new_special_tokens = None , special_tokens_map = None , ** kwargs ) The first step is to gather a training corpus. from datasets import load_dataset raw_datasets = load_dataset ( \"code_search_net\" , \"python\" ) def get_training_corpus (): dataset = raw_datasets [ \"train\" ] for start_idx in range ( 0 , len ( dataset ), 1000 ): samples = dataset [ start_idx : start_idx + 1000 ] yield samples [ \"whole_func_string\" ] Then, to train the tokenizer on this new corpus training_corpus = get_training_corpus () We can load GPT2 tokenizer architecture from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) The 4th line will train on new corpus. from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) new_tokenizer = old_tokenizer . train_new_from_iterator ( training_corpus , 52000 ) Once training is finished, we just have to save the tokenizer locally or send it to the hub. from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) new_tokenizer = old_tokenizer . train_new_from_iterator ( training_corpus , 52000 ) new_tokenizer . save_pretrained ( \"code-search-net-tokenizer\" ) And we can finally verify that our new tokenizer is more suitable for tokenizing python functions than the original GPT-2 tokenizer. example = \"\"\"class LinearLayer(): def __init__(self, input_size, output_size): self.weight = torch.randn(input_size, output_size) self.bias = torch.zeros(output_size) def __call__(self, x): return x @ self.weights + self.bias \"\"\" print ( old_tokenizer . tokenize ( example )) print ( new_tokenizer . tokenize ( example ))","title":"Training a new tokenizer from an old one"},{"location":"6_1TrainingNewTokenizerFromOld1/#training-a-new-tokenizer-from-an-old-one","text":"Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It\u2019s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It\u2019s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus.","title":"Training a new tokenizer from an old one"},{"location":"6_1TrainingNewTokenizerFromOld1/#training-a-new-tokenizer","text":"What is the purpose of training a tokenizer What are the key steps to follow What is the easiest way is to it. Should I train a new tokenizer when you plan to train a new model from scratch. You may want to consider training a new tokenizer so that you have a tokenizer suitable for the training corpus used to train a language model from scratch. A tokenizer will not be suitable if it has been trained on a corpus that is not similar to the one you will use to train your model from scratch. A trained tokenizer will not be suitable for your corpus if your corpus is in a different language, use new characters such as accent, etc, and a specific vocabulary, for instance medical or legal, or use a different style, eg, language of another country. Dissimilarities can arise from: New language New characters New domain New style If I take the tokenizer trained on bert-based-uncased model on ignore and normalized step, then we can say that the tokenization operations in the english sentence, \"here is a sentence adapted to our tokenizer\", produces a set of english tokens. from transformers import BertTokenizerFast tokenizer = BertTokenizerFast . from_pretrained ( 'huggingface-course/bert-base-uncased-tokenizer-without-normalizer' ) Here, a sentence of 8 words produces a tokens list of length 9. text = \"here is a sentence adapted to our tokenizer\" print ( tokenizer . tokenize ( text )) You may want to consider training a new tokenizer so that you have a tokenizer suitable for the training corpus used to train a language model from scratch. If I use the same tokenizer on the sentence in bengali, the words will be divided into may sub-tokens since tokenizes does not know the unicode characters. text = \"\u098f\u0987 \u09ac\u09be\u0995\u09cd\u09af\u099f\u09bf \u0986\u09ae\u09be\u09a6\u09c7\u09b0 \u099f\u09cb\u0995\u09c7\u09a8\u09be\u0987\u099c\u09be\u09b0\u09c7\u09b0 \u0989\u09aa\u09af\u09c1\u0995\u09cd\u09a4 \u09a8\u09af\u09bc\" print ( tokenizer . tokenize ( text )) The fact that the common word is split into many sub-tokens can be problemmatic. Because language models can only handle a sequence of tokens of limited length. The tokens that are excessively split the initial text may even impact the performance of the model. [UNK] tokens are also problemmatic because the model will not be able to extract any information from them. Another example, we see tokenizers replaces words containing characters on \u00e0cc\u00ebnts or CAPITAL LETTERS with [UNK] tokens text = \"this tokenizer does not know \u00e0cc\u00ebnts and CAPITAL LETTERS\" print ( tokenizer . tokenize ( text )) Finally, if we use this tokenizers to tokenize medical vocabulary, we see that a single word is divided into many smaller tokens. text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\" print ( tokenizer . tokenize ( text )) Most of the tokenizers used by the current State-of-the-Art language models need to be trained on a corpus that is allowed to one used to pre-train a language model. This training consists of learning rules to divide text into tokens On the way to learn these rules the token use them depends on the chosen tokenizer model. The procedural for training a tokenizer can be summarized in these main steps: Gathering a corpus of text Choosen a tokenizer architecture Train the tokenizer on the corpus Save the result Let's say you wanted to train a GPT-2 model on python code. example = \"\"\"class LinearLayer(): def __init__(self, input_size, output_size): self.weight = torch.randn(input_size, output_size) self.bias = torch.zeros(output_size) def __call__(self, x): return x @ self.weights + self.bias \"\"\" print ( old_tokenizer . tokenize ( example )) print ( new_tokenizer . tokenize ( example )) The Transformers library provides a very easy to use method to train a tokenizer using a known architecture on a new corpus. AutoTokenizer . train_new_from_iterator ( text_iterator , vocab_size , new_special_tokens = None , special_tokens_map = None , ** kwargs ) The first step is to gather a training corpus. from datasets import load_dataset raw_datasets = load_dataset ( \"code_search_net\" , \"python\" ) def get_training_corpus (): dataset = raw_datasets [ \"train\" ] for start_idx in range ( 0 , len ( dataset ), 1000 ): samples = dataset [ start_idx : start_idx + 1000 ] yield samples [ \"whole_func_string\" ] Then, to train the tokenizer on this new corpus training_corpus = get_training_corpus () We can load GPT2 tokenizer architecture from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) The 4th line will train on new corpus. from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) new_tokenizer = old_tokenizer . train_new_from_iterator ( training_corpus , 52000 ) Once training is finished, we just have to save the tokenizer locally or send it to the hub. from transformers import AutoTokenizer training_corpus = get_training_corpus () old_tokenizer = AutoTokenizer . from_pretrained ( \"gpt2\" ) new_tokenizer = old_tokenizer . train_new_from_iterator ( training_corpus , 52000 ) new_tokenizer . save_pretrained ( \"code-search-net-tokenizer\" ) And we can finally verify that our new tokenizer is more suitable for tokenizing python functions than the original GPT-2 tokenizer. example = \"\"\"class LinearLayer(): def __init__(self, input_size, output_size): self.weight = torch.randn(input_size, output_size) self.bias = torch.zeros(output_size) def __call__(self, x): return x @ self.weights + self.bias \"\"\" print ( old_tokenizer . tokenize ( example )) print ( new_tokenizer . tokenize ( example ))","title":"Training a new tokenizer"},{"location":"6_2FastTokenizersSpecialPowers/","text":"Fast tokenizers' special powers Why are fast tokenizers called fast? We will exactly how much faster the so-called fast tokenizers are compared to the slow tokenizers. Let's see how fast tokenizers are! Mnli dataset contains 432000 spares of text. from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mnli\" ) raw_datasets We will see how long it takes for the fast and slow versions of a bert tokenizer to process them all. We define two functions to preprocess the datasets. We define fast and slow tokenizer using AutoTokenizer api. from transformers import AutoTokenizer fast_tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) def tokenize_with_fast ( examples ): return fast_tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True ) The fast tokenizer is the default when available. So we pass along use_fast=False to define the slow one. slow_tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" , use_fast = False ) def tokenize_with_slow ( examples ): return fast_tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True ) Let's see an example of this on masked language modeling. In a notebook, we can time the execution of the cell with %time magic command. Processing the whole dataset is 4 times faster with fast tokenizer. That's better but very impressive. This is because we pass on text to the tokenizer one at a time. This is a common mistake to do with fast tokenizers which are backed by Rust. % time tokenized_datasets = raw_datasets . map ( tokenize_with_fast ) % time tokenized_datasets = raw_datasets . map ( tokenize_with_slow ) Properly using a fast tokenizer requires giving it multiple texts at the same time. Using fast tokenizers with batched=True is much, much faster. % time tokenized_datasets = raw_datasets . map ( tokenize_with_fast , batched = True ) % time tokenized_datasets = raw_datasets . map ( tokenize_with_slow , batched = True ) Fast tokenizer superpowers When performing tokenization, we lose some information. eg: here the tokenization is the same for below 2 sentences even if 1 has several more spaces than others. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) print ( tokenizer ( \"Let's talk about tokenizers superpowers.\" )[ \"input_ids\" ]) print ( tokenizer ( \"Let's talk about tokenizers superpowers.\" )[ \"input_ids\" ]) It is also difficult to know which word a token belongs to. It is difficult to know when 2 or more tokens belong to same word or not. Fast tokenizers keep track of the word each token comes from. encoding = tokenizer ( \"Let's talk about tokenizers superpowers.\" ) print ( encoding . tokens ()) print ( encoding . word_ids ()) They even keep track of each character span in the original text that gave each token. encoding = tokenizer ( \"Let's talk about tokenizers superpowers.\" , return_offsets_mapping = True ) print ( encoding . tokens ()) print ( encoding [ \"offset_mapping\" ]) The internal pipeline of the tokenizer looks like this. Normalization: \"Let's talk about tokenizers superpowers.\" Pre-tokenization: [Let,',s,talk,about,tokenizers,superpowers,.] Applying Model: [Let,',s,talk,about,token,##izer,##s,super,##power,##s,.] Special tokens: [[CLS],Let,',s,talk,about,token,##izer,##s,super,##power,##s,.,[SEP]] The fast tokenizers keep track of the original span of text creating each word or token. Here are a few applications of these features: Word IDs application: Whole word masking, Token classification Offset mapping application: Token classification, Question Answering Inside the Token classification pipeline (PyTorch) The token classification pipeline gives each token in the sentence a label. from transformers import pipeline token_classifier = pipeline ( \"token-classification\" ) token_classifier ( \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" ) It can also group together tokens corresponding to the same entity. token_classifier = pipeline ( \"token-classification\" , aggregation_strategy = \"simple\" ) token_classifier ( \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" ) The token classification pipeline follows the general steps of the pipeline we saw before. Tokenizer: Raw text -> Input IDs Model: Input IDs -> Logits Postprocessing: Logits -> Predictions We have already seen the first tseps of the pipeline: tokenization and model. from transformers import AutoTokenizer , AutoModelForTokenClassification model_checkpoint = \"\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) model = AutoModelForTokenClassification . from_pretrained ( model_checkpoint ) example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" inputs = tokenizer ( example , return_tensors = \"pt\" ) outputs = model ( ** inputs ) print ( inputs [ \"input_ids\" ] . shape ) # [1,19] print ( outputs . logits . shape ) # [1,19,9] The model outputs logits, which we need to convert to probabilities using softmax. We also get the predicted level for each token by taking the maximum prediction import torch probabilities = torch . nn . functional . softmax ( outputs . logits , dim =- 1 )[ 0 ] . tolist () predictions = probabilities . argmax ( dim =- 1 )[ 0 ] . tolist () print ( predictions ) The label correspondence then lets us match each prediction to a label. model . config . id2label The start and end character positions can be found using the offset mappings. results = [] inputs_with_offsets = tokenizer ( example , return_offsets_mapping = True ) tokens = inputs_with_offsets . tokens () offsets = inputs_with_offsets [ \"offset_mapping\" ] for idx , pred in enumerate ( predictions ): label = model . config . id2label [ pred ] if label != \"O\" : start , end = offsets [ idx ] results . append ( { \"entity\" : label , \"score\" : probabilities [ idx ][ pred ], \"word\" : tokens [ idx ], \"start\" : start , \"end\" : end } ) print ( results ) The last step is to group all the tokens corresponding to the same entity together. We have to group together in one entity all the corresponding labels. We group together tokens with the same label unless it's a B-XXX. import numpy as np label_map = model . config . id2label results = [] idx = 0 while idx < len ( predictions ): pred = predictions [ idx ] label = label_map [ pred ] if label != \"O\" : # Remove the B- or I- label = label [ 2 :] start , _ = offsets [ idx ] # Grab all the tokens labeled with I-label all_scores = [] while idx < len ( predictions ) and label_map [ predictions [ idx ]] == f \"I- { label } \" : all_scores . append ( probabilities [ idx ][ pred ]) _ , end = offsets [ idx ] idx += 1 # The score is the mean of all the scores of the token in that grouped entity. score = np . mean ( all_scores ) . item () word = example [ start : end ] results . append ( { \"entity_group\" : label , \"score\" : score , \"word\" : word , \"start\" : start , \"end\" : end } ) idx += 1","title":"Fast tokenizers special powers"},{"location":"6_2FastTokenizersSpecialPowers/#fast-tokenizers-special-powers","text":"","title":"Fast tokenizers' special powers"},{"location":"6_2FastTokenizersSpecialPowers/#why-are-fast-tokenizers-called-fast","text":"We will exactly how much faster the so-called fast tokenizers are compared to the slow tokenizers. Let's see how fast tokenizers are! Mnli dataset contains 432000 spares of text. from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mnli\" ) raw_datasets We will see how long it takes for the fast and slow versions of a bert tokenizer to process them all. We define two functions to preprocess the datasets. We define fast and slow tokenizer using AutoTokenizer api. from transformers import AutoTokenizer fast_tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) def tokenize_with_fast ( examples ): return fast_tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True ) The fast tokenizer is the default when available. So we pass along use_fast=False to define the slow one. slow_tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" , use_fast = False ) def tokenize_with_slow ( examples ): return fast_tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True ) Let's see an example of this on masked language modeling. In a notebook, we can time the execution of the cell with %time magic command. Processing the whole dataset is 4 times faster with fast tokenizer. That's better but very impressive. This is because we pass on text to the tokenizer one at a time. This is a common mistake to do with fast tokenizers which are backed by Rust. % time tokenized_datasets = raw_datasets . map ( tokenize_with_fast ) % time tokenized_datasets = raw_datasets . map ( tokenize_with_slow ) Properly using a fast tokenizer requires giving it multiple texts at the same time. Using fast tokenizers with batched=True is much, much faster. % time tokenized_datasets = raw_datasets . map ( tokenize_with_fast , batched = True ) % time tokenized_datasets = raw_datasets . map ( tokenize_with_slow , batched = True )","title":"Why are fast tokenizers called fast?"},{"location":"6_2FastTokenizersSpecialPowers/#fast-tokenizer-superpowers","text":"When performing tokenization, we lose some information. eg: here the tokenization is the same for below 2 sentences even if 1 has several more spaces than others. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) print ( tokenizer ( \"Let's talk about tokenizers superpowers.\" )[ \"input_ids\" ]) print ( tokenizer ( \"Let's talk about tokenizers superpowers.\" )[ \"input_ids\" ]) It is also difficult to know which word a token belongs to. It is difficult to know when 2 or more tokens belong to same word or not. Fast tokenizers keep track of the word each token comes from. encoding = tokenizer ( \"Let's talk about tokenizers superpowers.\" ) print ( encoding . tokens ()) print ( encoding . word_ids ()) They even keep track of each character span in the original text that gave each token. encoding = tokenizer ( \"Let's talk about tokenizers superpowers.\" , return_offsets_mapping = True ) print ( encoding . tokens ()) print ( encoding [ \"offset_mapping\" ]) The internal pipeline of the tokenizer looks like this. Normalization: \"Let's talk about tokenizers superpowers.\" Pre-tokenization: [Let,',s,talk,about,tokenizers,superpowers,.] Applying Model: [Let,',s,talk,about,token,##izer,##s,super,##power,##s,.] Special tokens: [[CLS],Let,',s,talk,about,token,##izer,##s,super,##power,##s,.,[SEP]] The fast tokenizers keep track of the original span of text creating each word or token. Here are a few applications of these features: Word IDs application: Whole word masking, Token classification Offset mapping application: Token classification, Question Answering","title":"Fast tokenizer superpowers"},{"location":"6_2FastTokenizersSpecialPowers/#inside-the-token-classification-pipeline-pytorch","text":"The token classification pipeline gives each token in the sentence a label. from transformers import pipeline token_classifier = pipeline ( \"token-classification\" ) token_classifier ( \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" ) It can also group together tokens corresponding to the same entity. token_classifier = pipeline ( \"token-classification\" , aggregation_strategy = \"simple\" ) token_classifier ( \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" ) The token classification pipeline follows the general steps of the pipeline we saw before. Tokenizer: Raw text -> Input IDs Model: Input IDs -> Logits Postprocessing: Logits -> Predictions We have already seen the first tseps of the pipeline: tokenization and model. from transformers import AutoTokenizer , AutoModelForTokenClassification model_checkpoint = \"\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) model = AutoModelForTokenClassification . from_pretrained ( model_checkpoint ) example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" inputs = tokenizer ( example , return_tensors = \"pt\" ) outputs = model ( ** inputs ) print ( inputs [ \"input_ids\" ] . shape ) # [1,19] print ( outputs . logits . shape ) # [1,19,9] The model outputs logits, which we need to convert to probabilities using softmax. We also get the predicted level for each token by taking the maximum prediction import torch probabilities = torch . nn . functional . softmax ( outputs . logits , dim =- 1 )[ 0 ] . tolist () predictions = probabilities . argmax ( dim =- 1 )[ 0 ] . tolist () print ( predictions ) The label correspondence then lets us match each prediction to a label. model . config . id2label The start and end character positions can be found using the offset mappings. results = [] inputs_with_offsets = tokenizer ( example , return_offsets_mapping = True ) tokens = inputs_with_offsets . tokens () offsets = inputs_with_offsets [ \"offset_mapping\" ] for idx , pred in enumerate ( predictions ): label = model . config . id2label [ pred ] if label != \"O\" : start , end = offsets [ idx ] results . append ( { \"entity\" : label , \"score\" : probabilities [ idx ][ pred ], \"word\" : tokens [ idx ], \"start\" : start , \"end\" : end } ) print ( results ) The last step is to group all the tokens corresponding to the same entity together. We have to group together in one entity all the corresponding labels. We group together tokens with the same label unless it's a B-XXX. import numpy as np label_map = model . config . id2label results = [] idx = 0 while idx < len ( predictions ): pred = predictions [ idx ] label = label_map [ pred ] if label != \"O\" : # Remove the B- or I- label = label [ 2 :] start , _ = offsets [ idx ] # Grab all the tokens labeled with I-label all_scores = [] while idx < len ( predictions ) and label_map [ predictions [ idx ]] == f \"I- { label } \" : all_scores . append ( probabilities [ idx ][ pred ]) _ , end = offsets [ idx ] idx += 1 # The score is the mean of all the scores of the token in that grouped entity. score = np . mean ( all_scores ) . item () word = example [ start : end ] results . append ( { \"entity_group\" : label , \"score\" : score , \"word\" : word , \"start\" : start , \"end\" : end } ) idx += 1","title":"Inside the Token classification pipeline (PyTorch)"},{"location":"6_3FastTokenizersInQApipeline/","text":"Fast tokenizers in QA pipeline Inside the Question answering pipeline (PyTorch) The question-answering pipeline finds the answer to questions in a given context. from transformers import pipeline question_answerer = pipeline ( \"question-answering\" ) context = \"\"\" \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question = \"Which deep learning libraries back \ud83e\udd17 Transformers?\" question_answerer ( question = question , context = context ) It also works for very long contexts. long_context = \"\"\" \ud83e\udd17 Transformers: State of the Art NLP \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models: - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint: 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime: - Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs: - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question_answerer ( question = question , context = long_context ) The question answering pipeline follows the general steps of the pipeline we saw before. We have already seen the first steps of the pipeline: tokenization and model. from transformers import AutoTokenizer , AutoModelForQuestionAnswering model_checkpoint = \"distilbert-base-cased-distilled-squad\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) model = AutoModelForQuestionAnswering . from_pretrained ( model_checkpoint ) inputs = tokenizer ( question , context , return_tensors = \"pt\" ) outputs = model ( ** inputs ) start_logits = outputs . start_logits end_logits = outputs . end_logits print ( start_logits . shape , end_logits . shape ) The tokens fed to the model will look like this. The answer is somewhere inside. So we assign labels like this-> start_logits and end_logits. Before applying the SoftMax, we mask the logits outside of the context. Belwo is what it looks like in terms of code. import torch sequence_ids = inputs . sequence_ids () # Mask everything apart the tokens of the context mask = [ i != 1 for i in sequence_ids ] # Unmask the [CLS] token mask [ 0 ] = False mask = torch . tensor ( mask )[ None ] start_logits [ mask ] = - 10000 end_logits [ mask ] = - 10000 start_probabilities = torch . nn . functional . softmax ( start_logits , dim =- 1 )[ 0 ] end_probabilities = torch . nn . functional . softmax ( end_logits , dim =- 1 )[ 0 ] An answer is a pair of start and end positions. score [ start_pos , end_pos ] = start_probabilities [ start_pos ] * end_probabilities [ end_pos ] After finding the possible answer with the best score, we use the offset mappings to find the corresponding answer in the context. scores = start_probabilities [:, None ] * end_probabilities [:, None ] scores = torch . triu ( scores ) max_index = scores . argmax () . item () start_index = max_index // scores . shape [ 1 ] end_index = max_index % scores . shape [ 1 ] score = scores [ start_index , end_index ] . item () inputs_with_offsets = tokenizer ( question , context , return_offsets_mapping = True ) offsets = inputs_with_offsets [ \"offset_mapping\" ] start_char , _ = offsets [ start_index ] _ , end_char = offsets [ end_index ] answer = context [ start_char : end_char ] print ( f \"Answer: ' { answer } ', score: { score : .4f } \" ) When the context is very long, it might get truncated by the tokenizer. Instead, we create several features for different pieces of the context... ...but to avoid truncating the answer, we allow some overlap. This is done automatically by the tokenizer if use the return_overflowing_tokens option. inputs = tokenizer ( question , long_context , stride = 128 , max_length = 384 , padding = \"longest\" , truncation = \"only_second\" , return_overflowing_tokens = True , return_offsets_mapping = True , ) We then select the most likely answer for each feature and the final answer is the one with the best score.","title":"Fast tokenizers in QA pipeline"},{"location":"6_3FastTokenizersInQApipeline/#fast-tokenizers-in-qa-pipeline","text":"","title":"Fast tokenizers in QA pipeline"},{"location":"6_3FastTokenizersInQApipeline/#inside-the-question-answering-pipeline-pytorch","text":"The question-answering pipeline finds the answer to questions in a given context. from transformers import pipeline question_answerer = pipeline ( \"question-answering\" ) context = \"\"\" \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question = \"Which deep learning libraries back \ud83e\udd17 Transformers?\" question_answerer ( question = question , context = context ) It also works for very long contexts. long_context = \"\"\" \ud83e\udd17 Transformers: State of the Art NLP \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models: - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint: 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime: - Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs: - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question_answerer ( question = question , context = long_context ) The question answering pipeline follows the general steps of the pipeline we saw before. We have already seen the first steps of the pipeline: tokenization and model. from transformers import AutoTokenizer , AutoModelForQuestionAnswering model_checkpoint = \"distilbert-base-cased-distilled-squad\" tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint ) model = AutoModelForQuestionAnswering . from_pretrained ( model_checkpoint ) inputs = tokenizer ( question , context , return_tensors = \"pt\" ) outputs = model ( ** inputs ) start_logits = outputs . start_logits end_logits = outputs . end_logits print ( start_logits . shape , end_logits . shape ) The tokens fed to the model will look like this. The answer is somewhere inside. So we assign labels like this-> start_logits and end_logits. Before applying the SoftMax, we mask the logits outside of the context. Belwo is what it looks like in terms of code. import torch sequence_ids = inputs . sequence_ids () # Mask everything apart the tokens of the context mask = [ i != 1 for i in sequence_ids ] # Unmask the [CLS] token mask [ 0 ] = False mask = torch . tensor ( mask )[ None ] start_logits [ mask ] = - 10000 end_logits [ mask ] = - 10000 start_probabilities = torch . nn . functional . softmax ( start_logits , dim =- 1 )[ 0 ] end_probabilities = torch . nn . functional . softmax ( end_logits , dim =- 1 )[ 0 ] An answer is a pair of start and end positions. score [ start_pos , end_pos ] = start_probabilities [ start_pos ] * end_probabilities [ end_pos ] After finding the possible answer with the best score, we use the offset mappings to find the corresponding answer in the context. scores = start_probabilities [:, None ] * end_probabilities [:, None ] scores = torch . triu ( scores ) max_index = scores . argmax () . item () start_index = max_index // scores . shape [ 1 ] end_index = max_index % scores . shape [ 1 ] score = scores [ start_index , end_index ] . item () inputs_with_offsets = tokenizer ( question , context , return_offsets_mapping = True ) offsets = inputs_with_offsets [ \"offset_mapping\" ] start_char , _ = offsets [ start_index ] _ , end_char = offsets [ end_index ] answer = context [ start_char : end_char ] print ( f \"Answer: ' { answer } ', score: { score : .4f } \" ) When the context is very long, it might get truncated by the tokenizer. Instead, we create several features for different pieces of the context... ...but to avoid truncating the answer, we allow some overlap. This is done automatically by the tokenizer if use the return_overflowing_tokens option. inputs = tokenizer ( question , long_context , stride = 128 , max_length = 384 , padding = \"longest\" , truncation = \"only_second\" , return_overflowing_tokens = True , return_offsets_mapping = True , ) We then select the most likely answer for each feature and the final answer is the one with the best score.","title":"Inside the Question answering pipeline (PyTorch)"},{"location":"6_4NormalizationPreTokenization/","text":"Normalization and pre-tokenization What is normalization? The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. Here is the result of the normalization of several tokenizers on the same sentence. FNetTokenizerFast . from_pretrained ( \"google/fnet-base\" ) RetriBertTokenizerFast . from_pretrained ( \"yjernite/retribert-base-uncased\" ) # and so on Fast tokenizers provide easy access to the normalization operation. from transformers import AutoTokenizer text = \"This is a text with \u00e0cc\u00ebnts and CAPITAL LETTERS\" tokenizer = AutoTokenizerFast . from_pretrained ( 'distilbert-base-uncased' ) print ( tokenizer . backend_tokenizer . normalizer . normalize_str ( text )) And it's really handy that the normalization operation is automatically included when you tokenize a text. # With saved normalizer from transformers import AutoTokenizer text = \"This is a text with \u00e0cc\u00ebnts and CAPITAL LETTERS\" tokenizer = AutoTokenizer . from_pretrained ( \"albert-large-v2\" ) print ( tokenizer . convert_ids_to_tokens ( tokenizer . encode ( text ))) # Without saved normalizer tokenizer = AutoTokenizer . from_pretrained ( \"huggingface-course/albert-tokenizer-without-normalizer\" ) print ( tokenizer . convert_ids_to_tokens ( tokenizer . encode ( text ))) Some normalizations may not be visible to the eye and but change many things for the computer. There are some Unicode normalization standards: NFC, NFD, NFKC and NFKD But beware, not all normalizations are suitable for all corpus. from transformers import AutoTokenizer text = \"un p\u00e8re indign\u00e9\" tokenizer = AutoTokenizerFast . from_pretrained ( 'distilbert-base-uncased' ) print ( tokenizer . backend_tokenizer . normalizer . normalize_str ( text )) What is pre-tokenization? The pre-tokenization applies rules to realize a first split of the text. Let's look at the result of the pre-tokenization of several tokenizers.: \"3.2.1: let's get started!\" 'gpt2': 3 . 2 . 1 : \u0120let 's \u0120get \u0120started ! 'albert-base-v1': _3.2.1: _let's _get _started! 'bert-base-uncased': 3 . 2 . 1 : let ' s get started ! Pre-tokenization can modify text - such as replacing a space with a special underscore - and split text into tokens. Fast tokenizers provide easy access to the pre-tokenization operation. from transformers import AutoTokenizerFast tokenizer = AutoTokenizerFast . from_pretrained ( 'albert-base-v1\u2019) text = \"3.2.1: let's get started!\" print ( tokenizer . backend_tokenizer . pre_tokenizer . pre_tokenize_str ( text ))","title":"Normalization and pre-tokenization"},{"location":"6_4NormalizationPreTokenization/#normalization-and-pre-tokenization","text":"","title":"Normalization and pre-tokenization"},{"location":"6_4NormalizationPreTokenization/#what-is-normalization","text":"The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. Here is the result of the normalization of several tokenizers on the same sentence. FNetTokenizerFast . from_pretrained ( \"google/fnet-base\" ) RetriBertTokenizerFast . from_pretrained ( \"yjernite/retribert-base-uncased\" ) # and so on Fast tokenizers provide easy access to the normalization operation. from transformers import AutoTokenizer text = \"This is a text with \u00e0cc\u00ebnts and CAPITAL LETTERS\" tokenizer = AutoTokenizerFast . from_pretrained ( 'distilbert-base-uncased' ) print ( tokenizer . backend_tokenizer . normalizer . normalize_str ( text )) And it's really handy that the normalization operation is automatically included when you tokenize a text. # With saved normalizer from transformers import AutoTokenizer text = \"This is a text with \u00e0cc\u00ebnts and CAPITAL LETTERS\" tokenizer = AutoTokenizer . from_pretrained ( \"albert-large-v2\" ) print ( tokenizer . convert_ids_to_tokens ( tokenizer . encode ( text ))) # Without saved normalizer tokenizer = AutoTokenizer . from_pretrained ( \"huggingface-course/albert-tokenizer-without-normalizer\" ) print ( tokenizer . convert_ids_to_tokens ( tokenizer . encode ( text ))) Some normalizations may not be visible to the eye and but change many things for the computer. There are some Unicode normalization standards: NFC, NFD, NFKC and NFKD But beware, not all normalizations are suitable for all corpus. from transformers import AutoTokenizer text = \"un p\u00e8re indign\u00e9\" tokenizer = AutoTokenizerFast . from_pretrained ( 'distilbert-base-uncased' ) print ( tokenizer . backend_tokenizer . normalizer . normalize_str ( text ))","title":"What is normalization?"},{"location":"6_4NormalizationPreTokenization/#what-is-pre-tokenization","text":"The pre-tokenization applies rules to realize a first split of the text. Let's look at the result of the pre-tokenization of several tokenizers.: \"3.2.1: let's get started!\" 'gpt2': 3 . 2 . 1 : \u0120let 's \u0120get \u0120started ! 'albert-base-v1': _3.2.1: _let's _get _started! 'bert-base-uncased': 3 . 2 . 1 : let ' s get started ! Pre-tokenization can modify text - such as replacing a space with a special underscore - and split text into tokens. Fast tokenizers provide easy access to the pre-tokenization operation. from transformers import AutoTokenizerFast tokenizer = AutoTokenizerFast . from_pretrained ( 'albert-base-v1\u2019) text = \"3.2.1: let's get started!\" print ( tokenizer . backend_tokenizer . pre_tokenizer . pre_tokenize_str ( text ))","title":"What is pre-tokenization?"},{"location":"6_5BytePairEncodingTokenization/","text":"Byte-Pair Encoding tokenization Byte-Pair algorithm was initially proposed as a text compression algorithm But it is also very well suited as a tokenizer for language models. The idea of BPE is to divide words into the sequence of 2 words units. BPE training is done on a standardized and pre-tokenized corpus. BPE training starts with an initial vocabulary and increses it to the desired size. To tokenize a text, it is sufficient to divide it into elementary units and then apply the merging rules successively.","title":"Byte-Pair Encoding tokenization"},{"location":"6_5BytePairEncodingTokenization/#byte-pair-encoding-tokenization","text":"Byte-Pair algorithm was initially proposed as a text compression algorithm But it is also very well suited as a tokenizer for language models. The idea of BPE is to divide words into the sequence of 2 words units. BPE training is done on a standardized and pre-tokenized corpus. BPE training starts with an initial vocabulary and increses it to the desired size. To tokenize a text, it is sufficient to divide it into elementary units and then apply the merging rules successively.","title":"Byte-Pair Encoding tokenization"},{"location":"6_6WordPieceTokenization/","text":"WordPiece tokenization WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. The learning strategy for a WordPiece tokenizer is similar to that of BPE but differs in the way the score for each candidate token is calculated. score =( freq_of_pair ) / ( freq_of_first_element X freq_of_second_element ) To tokenize a text with a learned WordPiece tokenizer we look for the longest token present at the beginning of the text.","title":"WordPiece tokenization"},{"location":"6_6WordPieceTokenization/#wordpiece-tokenization","text":"WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. The learning strategy for a WordPiece tokenizer is similar to that of BPE but differs in the way the score for each candidate token is calculated. score =( freq_of_pair ) / ( freq_of_first_element X freq_of_second_element ) To tokenize a text with a learned WordPiece tokenizer we look for the longest token present at the beginning of the text.","title":"WordPiece tokenization"},{"location":"6_7UnigramTokenization/","text":"Unigram tokenization The overall training strategy is to start with a very large vocabulary and then iteratively reduce it. Unigram model is a type of Statistical Language Model assuming that the occurrence of each word is independent of its previous word. Let's look at a toy example to understand how to train a Unigram LM tokenizer and how to use it to tokenize a new text. 1st iteration: E-step: Estimate the probabilities Additional explanations: How do we tokenize a text with Unigram LM? How do we calculate the loss on the training corpus? 1st iteration: M-step: Remove the token that least impacts the loss on the corpus. 2nd iteration: E-step: Estimate the probabilities 2nd iteration: M-step: Remove the token that least impacts the loss on the corpus. In practice, when we want to find the optimal tokenization of a word according to a Unigram model, we use the Viterbi algorithm instead of listing and calculating and comparing all the possibilities.","title":"Unigram tokenization"},{"location":"6_7UnigramTokenization/#unigram-tokenization","text":"The overall training strategy is to start with a very large vocabulary and then iteratively reduce it. Unigram model is a type of Statistical Language Model assuming that the occurrence of each word is independent of its previous word. Let's look at a toy example to understand how to train a Unigram LM tokenizer and how to use it to tokenize a new text. 1st iteration: E-step: Estimate the probabilities Additional explanations: How do we tokenize a text with Unigram LM? How do we calculate the loss on the training corpus? 1st iteration: M-step: Remove the token that least impacts the loss on the corpus. 2nd iteration: E-step: Estimate the probabilities 2nd iteration: M-step: Remove the token that least impacts the loss on the corpus. In practice, when we want to find the optimal tokenization of a word according to a Unigram model, we use the Viterbi algorithm instead of listing and calculating and comparing all the possibilities.","title":"Unigram tokenization"},{"location":"6_8BuildingATokenizer/","text":"Building a tokenizer, block by block Building a new tokenizer To build your tokenizer you need to design all its components: Normalization Pre-tokenization Model Post-Processing Decoding In a fast tokenizer all the components are gathered in the backend_tokenizer which is an instance of Tokenizer from the HuggingFace tokenizer library. from transformers import AutoTokenizerFast tokenizer = AutoTokenizerFast . from_pretrained ( \"...\" ) type ( tokenizer . backend_tokenizer ) tokenizers.Tokenizer <= Tokenizer from HuggingFace library. The main steps to create your own tokenizer: Gather a corpus Create a backend_tokenizer with HuggingFace tokenizers Load the backend_tokenizer in a HuggingFace transformers tokenizer Let's try to rebuild a BERT tokenizer together! 1. Gather a corpus from datasets import load_dataset dataset = load_dataset ( \"wikitext\" , name = \"wikitext-2-raw-v1\" , split = \"train\" ) def get_training_corpus (): for i in range ( 0 , len ( dataset ), 1000 ): yield dataset [ i : i + 1000 ][ \"text\" ] 2. Build a BERT tokenizer with HuggingFace tokenizers from tokenizers import Tokenizer , models , normalizers , pre_tokenizers , trainers , processors , decoders Initialize a model tokenizer = Tokenizer ( models . WordPiece ( unk_token = \"[UNK]\" )) Define a normalizer tokenizer . normalizer = normalizers . Sequence ( [ normalizers . Replace ( Regex ( r \"[\\p {Other} &&[^\\n\\t\\r]]\" ), \"\" ), normalizers . Replace ( Regex ( r \"[\\s]\" ), \" \" ), normalizers . Lowercase (), normalizers . NFD (), normalizers . StripAccents ()] ) Define pre-tokenization tokenizer . pre_tokenizer = pre_tokenizers . Whitespace () Define a trainer special_tokens = [ \"[UNK]\" , \"[PAD]\" , \"[CLS]\" , \"[SEP]\" , \"[MASK]\" ] trainer = trainers . WordPieceTrainer ( vocab_size = 25000 , special_tokens = special_tokens ) Train with an iterator tokenizer . train_from_iterator ( get_training_corpus (), trainer = trainer ) Define a template processing class cls_token_id = tokenizer . token_to_id ( \"[CLS]\" ) sep_token_id = tokenizer . token_to_id ( \"[SEP]\" ) tokenizer . post_processor = processors . TemplateProcessing ( single = f \"[CLS]:0 $A:0 [SEP]:0\" , pair = f \"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\" , special_tokens = [( \"[CLS]\" , cls_token_id ), ( \"[SEP]\" , sep_token_id )], ) Define a decoder tokenizer . decoder = decoders . WordPiece ( prefix = \"##\" ) Load your tokenizer into a HuggingFace transformers tokenizer","title":"Building a tokenizer, block by block"},{"location":"6_8BuildingATokenizer/#building-a-tokenizer-block-by-block","text":"","title":"Building a tokenizer, block by block"},{"location":"6_8BuildingATokenizer/#building-a-new-tokenizer","text":"To build your tokenizer you need to design all its components: Normalization Pre-tokenization Model Post-Processing Decoding In a fast tokenizer all the components are gathered in the backend_tokenizer which is an instance of Tokenizer from the HuggingFace tokenizer library. from transformers import AutoTokenizerFast tokenizer = AutoTokenizerFast . from_pretrained ( \"...\" ) type ( tokenizer . backend_tokenizer ) tokenizers.Tokenizer <= Tokenizer from HuggingFace library. The main steps to create your own tokenizer: Gather a corpus Create a backend_tokenizer with HuggingFace tokenizers Load the backend_tokenizer in a HuggingFace transformers tokenizer Let's try to rebuild a BERT tokenizer together!","title":"Building a new tokenizer"},{"location":"6_8BuildingATokenizer/#1-gather-a-corpus","text":"from datasets import load_dataset dataset = load_dataset ( \"wikitext\" , name = \"wikitext-2-raw-v1\" , split = \"train\" ) def get_training_corpus (): for i in range ( 0 , len ( dataset ), 1000 ): yield dataset [ i : i + 1000 ][ \"text\" ]","title":"1. Gather a corpus"},{"location":"6_8BuildingATokenizer/#2-build-a-bert-tokenizer-with-huggingface-tokenizers","text":"from tokenizers import Tokenizer , models , normalizers , pre_tokenizers , trainers , processors , decoders Initialize a model tokenizer = Tokenizer ( models . WordPiece ( unk_token = \"[UNK]\" )) Define a normalizer tokenizer . normalizer = normalizers . Sequence ( [ normalizers . Replace ( Regex ( r \"[\\p {Other} &&[^\\n\\t\\r]]\" ), \"\" ), normalizers . Replace ( Regex ( r \"[\\s]\" ), \" \" ), normalizers . Lowercase (), normalizers . NFD (), normalizers . StripAccents ()] ) Define pre-tokenization tokenizer . pre_tokenizer = pre_tokenizers . Whitespace () Define a trainer special_tokens = [ \"[UNK]\" , \"[PAD]\" , \"[CLS]\" , \"[SEP]\" , \"[MASK]\" ] trainer = trainers . WordPieceTrainer ( vocab_size = 25000 , special_tokens = special_tokens ) Train with an iterator tokenizer . train_from_iterator ( get_training_corpus (), trainer = trainer ) Define a template processing class cls_token_id = tokenizer . token_to_id ( \"[CLS]\" ) sep_token_id = tokenizer . token_to_id ( \"[SEP]\" ) tokenizer . post_processor = processors . TemplateProcessing ( single = f \"[CLS]:0 $A:0 [SEP]:0\" , pair = f \"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\" , special_tokens = [( \"[CLS]\" , cls_token_id ), ( \"[SEP]\" , sep_token_id )], ) Define a decoder tokenizer . decoder = decoders . WordPiece ( prefix = \"##\" ) Load your tokenizer into a HuggingFace transformers tokenizer","title":"2. Build a BERT tokenizer with HuggingFace tokenizers"},{"location":"7MainNLPTasks/","text":"Introduction we will tackle the following common NLP tasks: Token classification Masked language modeling (like BERT) Summarization Translation Causal language modeling pretraining (like GPT-2) Question answering","title":"Introduction"},{"location":"7MainNLPTasks/#introduction","text":"we will tackle the following common NLP tasks: Token classification Masked language modeling (like BERT) Summarization Translation Causal language modeling pretraining (like GPT-2) Question answering","title":"Introduction"}]}